import streamlit as st
import matplotlib # matplotlib ëª¨ë“ˆ ìì²´ë¥¼ ì„í¬íŠ¸
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from wordcloud import WordCloud
import io
import base64
from PIL import Image
import numpy as np
import os
import re
import requests
import json
import networkx as nx
from collections import Counter
import itertools
from pyvis.network import Network 

# í•œê¸€ í°íŠ¸ ì„¤ì •
FONT_PATH = "./fonts/Pretendard-Bold.ttf"

def setup_korean_font():
    """í•œê¸€ í°íŠ¸ë¥¼ matplotlibì— ì„¤ì •"""
    
    # Matplotlib ë²„ì „ê³¼ í°íŠ¸ ë§¤ë‹ˆì € ê²½ë¡œë¥¼ Streamlitì— ì¶œë ¥í•˜ì—¬ ì§„ë‹¨ì— ë„ì›€
    st.info(f"Matplotlib version check: {matplotlib.__version__}")
    st.info(f"Font Manager module path: {fm.__file__}")
    
    cache_dir = None
    try:
        cache_dir = fm.get_cachedir()
        st.info(f"Matplotlib font cache directory: {cache_dir}")
        
        # cache_dirì´ ì¡´ì¬í•˜ê³  ë””ë ‰í† ë¦¬ì¸ì§€ í™•ì¸
        if os.path.exists(cache_dir) and os.path.isdir(cache_dir):
            # 'fontlist-'ë¡œ ì‹œì‘í•˜ê³  '.json'ìœ¼ë¡œ ëë‚˜ëŠ” í°íŠ¸ ìºì‹œ íŒŒì¼ë“¤ì„ ì°¾ì•„ ì‚­ì œ
            for fname in os.listdir(cache_dir):
                if fname.startswith('fontlist-') and fname.endswith('.json'):
                    cache_file_path = os.path.join(cache_dir, fname)
                    try:
                        os.remove(cache_file_path)
                        st.info(f"Matplotlib í°íŠ¸ ìºì‹œ íŒŒì¼ ì‚­ì œ: {os.path.basename(cache_file_path)}") 
                    except OSError as e:
                        st.warning(f"í°íŠ¸ ìºì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({os.path.basename(cache_file_path)}): {e}")
        else:
            st.warning(f"Matplotlib í°íŠ¸ ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {cache_dir}")

    except AttributeError:
        st.error("ì˜¤ë¥˜: 'matplotlib.font_manager' ëª¨ë“ˆì— 'get_cachedir' ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤. Matplotlib ë²„ì „ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.info("ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ë©´ Matplotlibì„ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: pip install --upgrade matplotlib)")
    except Exception as e:
        st.warning(f"í°íŠ¸ ìºì‹œ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # í°íŠ¸ ë¡œë“œ ë° ì„¤ì •ì€ ê³„ì† ì‹œë„
    if os.path.exists(FONT_PATH):
        font_prop = fm.FontProperties(fname=FONT_PATH)
        plt.rcParams['font.family'] = font_prop.get_name()
        plt.rcParams['font.size'] = 10
        plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
        st.success(f"í•œê¸€ í°íŠ¸ '{font_prop.get_name()}'ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # ìƒˆë¡œìš´ í°íŠ¸ë¥¼ ë“±ë¡í–ˆìœ¼ë¯€ë¡œ í°íŠ¸ ë§¤ë‹ˆì €ì—ê²Œ ì—…ë°ì´íŠ¸ë¥¼ ì•Œë¦¼
        fm.findSystemFonts(fontpaths=None, rebuild_cache=True) 
        return True
    else:
        st.warning(f"í•œê¸€ í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {FONT_PATH}")
        st.info("`/fonts/Pretendard-Bold.ttf` ê²½ë¡œì— í°íŠ¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

# ì•± ì‹œì‘ ì‹œ í°íŠ¸ ì„¤ì •
setup_korean_font()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="GPT API í‚¤ì›Œë“œ ì¶”ì¶œ & ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°",
    page_icon="ğŸ¤–",
    layout="wide"
)

def call_openai_api(text, api_key, model="gpt-4o-mini"): 
    """OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    
    prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•˜ê³  ì¤‘ìš”ë„ì— ë”°ë¼ 1~10 ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text}"

ìš”êµ¬ì‚¬í•­:
1. 10~20ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
2. ê° í‚¤ì›Œë“œì— 1~10 ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ ë¶€ì—¬ (10ì´ ê°€ì¥ ì¤‘ìš”)
3. ë¶ˆìš©ì–´(ì¡°ì‚¬, ì–´ë¯¸, ì¼ë°˜ì ì¸ ë‹¨ì–´) ì œì™¸
4. ë³µí•©ì–´ë‚˜ ì¤‘ìš”í•œ êµ¬ë¬¸ë„ í¬í•¨ ê°€ëŠ¥
5. ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:

í‚¤ì›Œë“œA 5, í‚¤ì›Œë“œB 4, í‚¤ì›Œë“œC 3

ìœ„ í˜•ì‹ ì™¸ì˜ ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
"""

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [
            {
                "role": "user", 
                "content": prompt
            }
        ],
        "max_tokens": 500,
        "temperature": 0.3
    }
    
    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
        else:
            return f"API ì˜¤ë¥˜: {response.status_code} - {response.text}"
            
    except requests.exceptions.RequestException as e:
        return f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}"
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def parse_gpt_response(response_text):
    """GPT ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
    keywords_dict = {}
    
    try:
        items = response_text.split(',')
        
        for item in items:
            item = item.strip()
            if not item: 
                continue
            
            parts = item.rsplit(' ', 1)
            
            if len(parts) == 2:
                keyword = parts[0].strip()
                try:
                    weight = int(parts[1].strip())
                    if 1 <= weight <= 10: 
                        keywords_dict[keyword] = weight
                    else: 
                        keywords_dict[keyword] = 5 
                except ValueError: 
                    keywords_dict[item] = 5
            else: 
                keywords_dict[item] = 5
                
        return keywords_dict
        
    except Exception as e:
        st.error(f"ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {str(e)}. ì‘ë‹µ: '{response_text}'")
        return {}

def create_wordcloud_from_keywords(keywords_dict, width=800, height=600, bg_color='white'):
    """í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬ë¡œë¶€í„° ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    if not keywords_dict:
        return None
    
    if not os.path.exists(FONT_PATH):
        st.error(f"í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FONT_PATH}")
        st.info("í°íŠ¸ íŒŒì¼ì´ `/fonts/Pretendard-Bold.ttf` ê²½ë¡œì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    
    try:
        wc = WordCloud(
            font_path=FONT_PATH, 
            width=width,
            height=height,
            background_color=bg_color,
            max_words=100,
            relative_scaling=0.5,
            min_font_size=10,
            colormap='viridis',
            prefer_horizontal=0.7
        ).generate_from_frequencies(keywords_dict)
        
        return wc
    except Exception as e:
        st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None

def wordcloud_to_image(wc, width=800, height=600):
    """ì›Œë“œí´ë¼ìš°ë“œë¥¼ PIL Imageë¡œ ë³€í™˜"""
    fig, ax = plt.subplots(figsize=(width/100, height/100)) 
    ax.imshow(wc, interpolation='bilinear')
    ax.axis('off')
    
    buf = io.BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0, dpi=100)
    buf.seek(0)
    plt.close(fig) 
    
    image = Image.open(buf)
    return image

def get_image_download_link(img, filename, link_text):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„± (PNG)"""
    buffered = io.BytesIO()
    img.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    href = f'<a href="data:file/png;base64,{img_str}" download="{filename}">{link_text}</a>'
    return href

# pyvis HTML ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ í•¨ìˆ˜
def get_html_download_link(html_content, filename, link_text):
    """HTML ë‚´ìš© ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±"""
    b64 = base64.b64encode(html_content.encode()).decode()
    href = f'<a href="data:text/html;base64,{b64}" download="{filename}">{link_text}</a>'
    return href


def create_network_analysis(text, keywords_dict, min_cooccurrence=1):
    """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ê°„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ (networkx ê·¸ë˜í”„ ìƒì„±)"""
    if not keywords_dict or len(keywords_dict) < 2:
        return None, None
    
    sentences = re.split(r'[.!?]\s*', text)
    
    co_occurrence = Counter()
    keyword_list = list(keywords_dict.keys())
    
    for sentence in sentences:
        found_keywords = []
        sentence_lower = sentence.lower()
        for keyword in keyword_list:
            if keyword.lower() in sentence_lower: 
                found_keywords.append(keyword)
        
        for combo in itertools.combinations(found_keywords, 2):
            sorted_combo = tuple(sorted(combo))
            co_occurrence[sorted_combo] += 1
    
    filtered_co_occurrence = {k: v for k, v in co_occurrence.items() if v >= min_cooccurrence}

    if not filtered_co_occurrence:
        return None, None
    
    G = nx.Graph()
    
    nodes_to_add = set()
    for (k1, k2) in filtered_co_occurrence.keys():
        nodes_to_add.add(k1)
        nodes_to_add.add(k2)

    for keyword in nodes_to_add:
        weight = keywords_dict.get(keyword, 1) 
        G.add_node(keyword, weight=weight)
    
    for (keyword1, keyword2), freq in filtered_co_occurrence.items():
        G.add_edge(keyword1, keyword2, weight=freq)
    
    if G.number_of_nodes() < 2: 
        return None, None

    return G, filtered_co_occurrence 

# ê¸°ì¡´ draw_network_graphë¥¼ ëŒ€ì²´í•˜ëŠ” pyvis í•¨ìˆ˜
def create_pyvis_network_html(G, keywords_dict):
    """networkx ê·¸ë˜í”„ë¡œë¶€í„° pyvis ëŒ€í™”í˜• ë„¤íŠ¸ì›Œí¬ ìƒì„± ë° HTML ë°˜í™˜"""
    if G is None or len(G.nodes()) < 2:
        return None
    
    # pyvis Network ê°ì²´ ì´ˆê¸°í™”
    net = Network(notebook=False, height="750px", width="100%", 
                  cdn_resources='remote', 
                  directed=False) 

    # ë…¸ë“œ ì¶”ê°€
    korean_font_family = "'Pretendard Bold', 'Malgun Gothic', 'Apple SD Gothic Neo', sans-serif"

    for node, attrs in G.nodes(data=True):
        size = keywords_dict.get(node, 1) * 7 
        title = f"í‚¤ì›Œë“œ: {node}<br>ê°€ì¤‘ì¹˜: {keywords_dict.get(node, 1)}" 
        
        net.add_node(
            node, 
            label=node, 
            size=size, 
            title=title,
            color='#6A0DAD', 
            font={'size': 12, 'color': 'black', 'face': korean_font_family, 'align': 'center'}
        )

    # ì—£ì§€ ì¶”ê°€
    for u, v, attrs in G.edges(data=True):
        weight = attrs.get('weight', 1) 
        title = f"ë™ì‹œì¶œí˜„: {weight}íšŒ" 
        net.add_edge(
            u, v, 
            value=weight, 
            title=title,
            color='gray',
            width=weight/2 
        )

    # ë¬¼ë¦¬ ì—”ì§„ ì„¤ì •
    net.set_options("""
    var options = {
      "physics": {
        "forceAtlas2Based": {
          "gravitationalConstant": -50,
          "centralGravity": 0.005,
          "springLength": 200,
          "springConstant": 0.08,
          "avoidOverlap": 0.5
        },
        "minVelocity": 0.75,
        "solver": "forceAtlas2Based"
      },
      "nodes": {
          "borderWidth": 2,
          "borderWidthSelected": 4
      },
      "edges": {
          "smooth": {
              "enabled": true,
              "type": "dynamic",
              "roundness": 0.5
          }
      },
      "interaction": {
        "hover": true,
        "zoomView": true,
        "navigationButtons": true
      }
    }
    """)

    # ê·¸ë˜í”„ë¥¼ ì„ì‹œ HTML íŒŒì¼ë¡œ ì €ì¥
    temp_html_file = "pyvis_network.html"
    net.save_graph(temp_html_file)

    # ì €ì¥ëœ HTML íŒŒì¼ì„ ì½ì–´ì™€ì„œ ë°˜í™˜
    with open(temp_html_file, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    os.remove(temp_html_file)

    return html_content


def analyze_network_metrics(G, keywords_dict):
    """ë„¤íŠ¸ì›Œí¬ ì§€í‘œ ë¶„ì„"""
    if G is None or len(G.nodes()) < 2:
        return {}
    
    metrics = {}
    
    metrics['nodes'] = G.number_of_nodes()
    metrics['edges'] = G.number_of_edges()
    
    if nx.is_connected(G):
        metrics['density'] = nx.density(G)
        metrics['components'] = 1
    else:
        metrics['density'] = "ê·¸ë˜í”„ê°€ ì—°ê²°ë˜ì–´ ìˆì§€ ì•ŠìŒ (ë°€ë„ ê³„ì‚° ë¶ˆê°€)"
        metrics['components'] = nx.number_connected_components(G)
    
    try:
        degree_centrality = nx.degree_centrality(G)
        betweenness_centrality = nx.betweenness_centrality(G)
        closeness_centrality = nx.closeness_centrality(G)
        
        metrics['most_connected'] = max(degree_centrality, key=degree_centrality.get) if degree_centrality else "N/A"
        metrics['most_between'] = max(betweenness_centrality, key=betweenness_centrality.get) if betweenness_centrality else "N/A"
        metrics['most_close'] = max(closeness_centrality, key=closeness_centrality.get) if closeness_centrality else "N/A"
        
        metrics['degree_top5'] = dict(sorted(degree_centrality.items(), 
                                           key=lambda x: x[1], reverse=True)[:5])
        metrics['betweenness_top5'] = dict(sorted(betweenness_centrality.items(), 
                                                key=lambda x: x[1], reverse=True)[:5])
        metrics['closeness_top5'] = dict(sorted(closeness_centrality.items(), 
                                                key=lambda x: x[1], reverse=True)[:5])
        
    except Exception as e:
        st.warning(f"ë„¤íŠ¸ì›Œí¬ ì¤‘ì‹¬ì„± ì§€í‘œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        metrics['most_connected'] = "ê³„ì‚° ë¶ˆê°€"
        metrics['most_between'] = "ê³„ì‚° ë¶ˆê°€"
        metrics['most_close'] = "ê³„ì‚° ë¶ˆê°€"
        metrics['degree_top5'] = {}
        metrics['betweenness_top5'] = {}
        metrics['closeness_top5'] = {}
    
    return metrics


# ì•± ì œëª© ë° ì„¤ëª…
st.title("ğŸ¤– GPT API í‚¤ì›Œë“œ ì¶”ì¶œ & ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°")
st.markdown("GPT APIë¥¼ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤!")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("ğŸ”‘ API ì„¤ì •")

# API í‚¤ ì…ë ¥
api_key = st.sidebar.text_input(
    "OpenAI API Key", 
    type="password",
    help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (https://platform.openai.com/api-keys)"
)

model_choice = st.sidebar.selectbox(
    "GPT ëª¨ë¸",
    ["gpt-4o-mini", "gpt-3.5-turbo"], 
    index=0,
    help="ì‚¬ìš©í•  GPT ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”. gpt-4o-miniê°€ ê°€ì¥ ê²½ì œì ì´ë©´ì„œë„ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤."
)

# ì›Œë“œí´ë¼ìš°ë“œ ì„¤ì •
st.sidebar.header("ğŸ¨ ì‹œê°í™” ì„¤ì •")
wc_width = st.sidebar.slider("ì›Œë“œí´ë¼ìš°ë“œ ë„ˆë¹„", 400, 1200, 800)
wc_height = st.sidebar.slider("ì›Œë“œí´ë¼ìš°ë“œ ë†’ì´", 300, 800, 600)
bg_color = st.sidebar.selectbox("ë°°ê²½ìƒ‰", ["white", "black"], index=0)

# ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì„¤ì •
show_network = st.sidebar.checkbox("ğŸŒ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í¬í•¨", value=True)
min_cooccurrence = st.sidebar.slider("ìµœì†Œ ë™ì‹œì¶œí˜„ íšŸìˆ˜", 1, 5, 1, 
                                    help="ì´ ê°’ ì´ìƒìœ¼ë¡œ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œë“¤ë§Œ ì—°ê²°ì„ ìœ¼ë¡œ í‘œì‹œ")

# API í‚¤ í™•ì¸
if not api_key:
    st.warning("âš ï¸ OpenAI API í‚¤ë¥¼ ì™¼ìª½ ì‚¬ì´ë“œë°”ì— ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.info("""
    **API í‚¤ ë°œê¸‰ ë°©ë²•:**
    1. https://platform.openai.com ì ‘ì†
    2. ë¡œê·¸ì¸ í›„ 'API Keys' ë©”ë‰´ë¡œ ì´ë™
    3. 'Create new secret key' í´ë¦­
    4. ìƒì„±ëœ í‚¤ë¥¼ ë³µì‚¬í•˜ì—¬ ì‚¬ì´ë“œë°”ì— ì…ë ¥
    
    **ì£¼ì˜ì‚¬í•­:**
    - API ì‚¬ìš©ë£Œê°€ ë¶€ê³¼ë©ë‹ˆë‹¤
    - í‚¤ëŠ” ì•ˆì „í•˜ê²Œ ë³´ê´€í•˜ì„¸ìš”
    
    **í°íŠ¸ íŒŒì¼ ì„¤ì •:**
    - `fonts/Pretendard-Bold.ttf` íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤. `/fonts` ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•˜ê³  ê·¸ ì•ˆì— ë„£ì–´ì£¼ì„¸ìš”.
    """)

# ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
st.header("ğŸ“ í…ìŠ¤íŠ¸ ì…ë ¥")

with st.expander("ğŸ’¡ ì‚¬ìš©ë²• ë° ì˜ˆì‹œ"):
    st.markdown("""
    **ì‚¬ìš©ë²•:**
    1. OpenAI API í‚¤ë¥¼ ì‚¬ì´ë“œë°”ì— ì…ë ¥
    2. ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥
    3. 'GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ' ë²„íŠ¼ í´ë¦­
    4. ì¶”ì¶œëœ í‚¤ì›Œë“œ, ì›Œë“œí´ë¼ìš°ë“œ ë° ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ê²°ê³¼ í™•ì¸
    
    **ì˜ˆì‹œ í…ìŠ¤íŠ¸:**
    ```
    ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ ë¶„ì•¼ì…ë‹ˆë‹¤. 
    ë°ì´í„° ê³¼í•™ìë“¤ì€ ë¹…ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ íŒ¨í„´ì„ ì°¾ê³ , 
    ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. 
    ìì—°ì–´ ì²˜ë¦¬ì™€ ì»´í“¨í„° ë¹„ì „ ê¸°ìˆ ì´ ë°œì „í•˜ë©´ì„œ 
    ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì—ì„œ í˜ì‹ ì´ ì¼ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤.
    ```
    
    **GPTì˜ ì¥ì :**
    - ë¬¸ë§¥ì„ ì´í•´í•œ ì •í™•í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
    - ë™ì˜ì–´/ìœ ì˜ì–´ ê·¸ë£¹í•‘
    - ì¤‘ìš”ë„ì— ë”°ë¥¸ ì •êµí•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    - ë³µí•©ì–´ì™€ ì „ë¬¸ìš©ì–´ ì¸ì‹
    """)

text_input = st.text_area(
    "ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
    height=200,
    placeholder="ì˜ˆì‹œ: ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ ë¶„ì•¼ì…ë‹ˆë‹¤...",
    help="GPTê°€ ì´ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."
)

if st.button("ğŸ¤– GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ", type="primary", disabled=not api_key):
    if not api_key:
        st.error("OpenAI API í‚¤ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not text_input.strip():
        st.warning("ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner(f"{model_choice} ëª¨ë¸ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
            gpt_response = call_openai_api(text_input, api_key, model_choice)
            
            if gpt_response.startswith("API ì˜¤ë¥˜") or gpt_response.startswith("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜") or gpt_response.startswith("ì˜¤ë¥˜ ë°œìƒ"):
                st.error(gpt_response)
            else:
                st.success("í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
                
                st.header("ğŸ¤– GPT ì‘ë‹µ")
                st.code(gpt_response, language=None)
                
                keywords_dict = parse_gpt_response(gpt_response)
                
                if keywords_dict:
                    st.header("ğŸ“Š ì¶”ì¶œëœ í‚¤ì›Œë“œ")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.subheader("ğŸ¯ í‚¤ì›Œë“œ ëª©ë¡")
                        display_keywords = {k: v for k, v in keywords_dict.items() if v > 0}
                        if display_keywords:
                            for keyword, weight in sorted(display_keywords.items(), key=lambda x: x[1], reverse=True):
                                st.write(f"**{keyword}**: {weight}")
                        else:
                            st.info("ì¶”ì¶œëœ ìœ íš¨í•œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

                    with col2:
                        st.subheader("ğŸ“ˆ ê°€ì¤‘ì¹˜ ë¶„í¬")
                        weights = list(keywords_dict.values())
                        
                        fig, ax = plt.subplots(figsize=(6, 4))
                        ax.hist(weights, bins=range(1, 12), alpha=0.7, color='skyblue', edgecolor='black')
                        
                        if os.path.exists(FONT_PATH):
                            font_prop = fm.FontProperties(fname=FONT_PATH)
                            ax.set_xlabel('ê°€ì¤‘ì¹˜', fontproperties=font_prop)
                            ax.set_ylabel('í‚¤ì›Œë“œ ìˆ˜', fontproperties=font_prop)
                            ax.set_title('í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶„í¬', fontproperties=font_prop)
                        else:
                            ax.set_xlabel('Weight')
                            ax.set_ylabel('Keywords Count')
                            ax.set_title('Keyword Weight Distribution')
                            
                        ax.set_xticks(range(1, 11))
                        ax.grid(True, alpha=0.3)
                        st.pyplot(fig)
                        plt.close(fig) 
                    
                    st.subheader("ğŸ“‹ ë³µì‚¬ìš© ê²°ê³¼")
                    st.text_area(
                        "ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹", 
                        value=gpt_response, 
                        height=100,
                        help="ì´ í…ìŠ¤íŠ¸ë¥¼ ë³µì‚¬í•´ì„œ ë‹¤ë¥¸ ì›Œë“œí´ë¼ìš°ë“œ ë„êµ¬ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )
                    
                    st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
                    
                    with st.spinner("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ëŠ” ì¤‘..."):
                        wordcloud = create_wordcloud_from_keywords(keywords_dict, wc_width, wc_height, bg_color)
                        
                        if wordcloud:
                            img = wordcloud_to_image(wordcloud, wc_width, wc_height)
                            st.image(img, caption="GPTë¡œ ìƒì„±ëœ ì›Œë“œí´ë¼ìš°ë“œ", use_column_width=True)
                            
                            download_link = get_image_download_link(img, "gpt_wordcloud.png", "ğŸ“¥ ì›Œë“œí´ë¼ìš°ë“œ ë‹¤ìš´ë¡œë“œ")
                            st.markdown(download_link, unsafe_allow_html=True)
                            
                            st.info(f"ëª¨ë¸: {model_choice} | í¬ê¸°: {wc_width}x{wc_height} | í‚¤ì›Œë“œ ìˆ˜: {len(keywords_dict)}ê°œ")
                
                else:
                    st.error("GPT ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

                if show_network:
                    st.header("ğŸŒ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
                    st.markdown("**Tip**: ë§ˆìš°ìŠ¤ë¡œ ë…¸ë“œë¥¼ ëŒê±°ë‚˜ í™•ëŒ€/ì¶•ì†Œí•˜ì—¬ ëŒ€í™”í˜• ê·¸ë˜í”„ë¥¼ íƒìƒ‰í•´ë³´ì„¸ìš”!")
                    if len(keywords_dict) < 2:
                        st.warning("ë„¤íŠ¸ì›Œí¬ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ í‚¤ì›Œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                    else:
                        with st.spinner("í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¶„ì„í•˜ê³  ê·¸ë¦¬ëŠ” ì¤‘... (ëŒ€í™”í˜•)"):
                            G, co_occurrence_data = create_network_analysis(text_input, keywords_dict, min_cooccurrence)
                            
                            if G and G.number_of_nodes() >= 2:
                                pyvis_html = create_pyvis_network_html(G, keywords_dict)
                                if pyvis_html:
                                    # HTML ì»´í¬ë„ŒíŠ¸ ì¶œë ¥
                                    st.components.v1.html(pyvis_html, height=750, scrolling=True) 
                                    
                                    # HTML íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬
                                    network_download_link = get_html_download_link(pyvis_html, "keyword_network.html", "ğŸ“¥ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ë‹¤ìš´ë¡œë“œ (HTML)")
                                    st.markdown(network_download_link, unsafe_allow_html=True)

                                    st.subheader("ğŸ“Š ë„¤íŠ¸ì›Œí¬ ì§€í‘œ")
                                    metrics = analyze_network_metrics(G, keywords_dict)
                                    if metrics:
                                        st.write(f"**ì´ ë…¸ë“œ ìˆ˜ (í‚¤ì›Œë“œ):** {metrics.get('nodes', 'N/A')}ê°œ")
                                        st.write(f"**ì´ ì—£ì§€ ìˆ˜ (ì—°ê²°):** {metrics.get('edges', 'N/A')}ê°œ")
                                        st.write(f"**ê·¸ë˜í”„ ë°€ë„:** {metrics.get('density', 'N/A'):.4f}" if isinstance(metrics.get('density'), float) else f"**ê·¸ë˜í”„ ë°€ë„:** {metrics.get('density', 'N/A')}")
                                        st.write(f"**ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸ ìˆ˜:** {metrics.get('components', 'N/A')}ê°œ")
                                        
                                        st.markdown("---")
                                        st.subheader("â­ ì£¼ìš” í‚¤ì›Œë“œ (ì¤‘ì‹¬ì„± ì§€í‘œ)")
                                        st.markdown("**ì—°ê²° ì¤‘ì‹¬ì„± (Degree Centrality):** ê°€ì¥ ë§ì€ í‚¤ì›Œë“œì™€ ì—°ê²°ëœ í‚¤ì›Œë“œ (ì˜í–¥ë ¥)")
                                        if metrics.get('degree_top5'):
                                            for k, v in metrics['degree_top5'].items():
                                                st.write(f"- **{k}**: {v:.4f}")
                                        else:
                                            st.write("ì •ë³´ ì—†ìŒ")

                                        st.markdown("**ë§¤ê°œ ì¤‘ì‹¬ì„± (Betweenness Centrality):** ë‹¤ë¥¸ í‚¤ì›Œë“œë“¤ ì‚¬ì´ì˜ ë‹¤ë¦¬ ì—­í• ì„ í•˜ëŠ” í‚¤ì›Œë“œ (ì¤‘ê°œì ì—­í• )")
                                        if metrics.get('betweenness_top5'):
                                            for k, v in metrics['betweenness_top5'].items():
                                                st.write(f"- **{k}**: {v:.4f}")
                                        else:
                                            st.write("ì •ë³´ ì—†ìŒ")
                                            
                                        st.markdown("**ê·¼ì ‘ ì¤‘ì‹¬ì„± (Closeness Centrality):** ë‹¤ë¥¸ í‚¤ì›Œë“œì— ì–¼ë§ˆë‚˜ ê°€ê¹ê²Œ ë„ë‹¬í•  ìˆ˜ ìˆëŠ”ì§€ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œ (ì •ë³´ í™•ì‚° ìš©ì´ì„±)")
                                        if metrics.get('closeness_top5'):
                                            for k, v in metrics['closeness_top5'].items():
                                                st.write(f"- **{k}**: {v:.4f}")
                                        else:
                                            st.write("ì •ë³´ ì—†ìŒ")

                                    st.subheader("â†”ï¸ í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¹ˆë„ (ìƒìœ„ 10ê°œ)")
                                    if co_occurrence_data:
                                        sorted_co_occurrence = sorted(co_occurrence_data.items(), key=lambda item: item[1], reverse=True)[:10]
                                        for (k1, k2), freq in sorted_co_occurrence:
                                            st.write(f"- **{k1}**ì™€ **{k2}**: {freq}íšŒ")
                                    else:
                                        st.info("ë™ì‹œì¶œí˜„ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ìµœì†Œ ë™ì‹œì¶œí˜„ íšŸìˆ˜ ì„¤ì •ì´ ë„ˆë¬´ ë†’ìŠµë‹ˆë‹¤.")
                                else:
                                    st.error("ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                            else:
                                st.info(f"ë„¤íŠ¸ì›Œí¬ë¥¼ ê·¸ë¦´ ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ í‚¤ì›Œë“œ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤. (í˜„ì¬ ë…¸ë“œ ìˆ˜: {G.number_of_nodes() if G else 0})")
                                if G and G.number_of_nodes() > 0:
                                    st.info(f"ì„ íƒëœ ìµœì†Œ ë™ì‹œì¶œí˜„ íšŸìˆ˜ ({min_cooccurrence}íšŒ) ì´ìƒìœ¼ë¡œ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì„¤ì •ì„ ë‚®ì¶°ë³´ì„¸ìš”.")

if api_key:
    st.sidebar.markdown("---")
    st.sidebar.header("ğŸ’° ë¹„ìš© ì•ˆë‚´")
    st.sidebar.markdown("""
    **ì˜ˆìƒ ë¹„ìš© (1íšŒ ìš”ì²­):**
    - `gpt-4o-mini`: ~$0.000004/token (ì…ë ¥), ~$0.000012/token (ì¶œë ¥)
    - `gpt-3.5-turbo`: ~$0.0000005/token (ì…ë ¥), ~$0.0000015/token (ì¶œë ¥)
    
    *ì‹¤ì œ ë¹„ìš©ì€ ì…ë ¥ ë° ì¶œë ¥ í† í° ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.*
    *`gpt-4o-mini`ê°€ `gpt-3.5-turbo`ë³´ë‹¤ ë¹„ì‹¸ì§€ë§Œ, í›¨ì”¬ ë” ë†’ì€ í’ˆì§ˆì˜ í‚¤ì›Œë“œ ì¶”ì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.*
    """)

st.markdown("---")
st.markdown("ğŸ’¡ **íŒ**: GPTê°€ ë¬¸ë§¥ì„ ì´í•´í•˜ë¯€ë¡œ ë” ì •í™•í•˜ê³  ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
st.markdown("ğŸ¤– GPT API í‚¤ì›Œë“œ ì¶”ì¶œê¸° | Made with Streamlit")