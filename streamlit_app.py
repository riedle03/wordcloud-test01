import streamlit as st
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import io
import base64
from PIL import Image
import numpy as np
import os
import re
from collections import Counter
import math

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ & ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°",
    page_icon="ğŸ”",
    layout="wide"
)

# í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (êµìœ¡ ë¶„ì•¼ì— ë§ê²Œ ì¡°ì •)
KOREAN_STOPWORDS = {
    'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ë“¤', 'ëŠ”', 'ì€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ê°€', 'ì™€', 'ê³¼', 'ë„', 'ë¡œ', 'ìœ¼ë¡œ',
    'ì´ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ê°™ë‹¤', 'ë‹¤ë¥¸', 'ë§ì€', 'ì‘ì€', 'í°', 'ì¢‹ì€', 'ë‚˜ìœ',
    'ë˜í•œ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ì¦‰', 'ì˜ˆë¥¼', 'ë“¤ì–´', 'ë°”ë¡œ', 'ë‹¨ì§€', 'ë‹¤ë§Œ',
    'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•´', 'ê´€í•´', 'ì—ì„œ', 'ì—ê²Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë¼ê³ ', 'ë¼ëŠ”', 'ì´ë¼ëŠ”',
    'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ì§€ê¸ˆ', 'ì˜¤ëŠ˜',
    'ë‚´ì¼', 'ì–´ì œ', 'ì–¸ì œ', 'ì–´ë””', 'ëˆ„êµ¬', 'ë¬´ì—‡', 'ì™œ', 'ì–´ë–»ê²Œ', 'ì–´ë–¤', 'ëª¨ë“ ', 'ê°ê°',
    'ìˆ˜', 'ë•Œ', 'ê³³', 'ì‚¬ëŒ', 'ê²ƒë“¤', 'ì ', 'ë©´', 'ë“±', 'ì¤‘', 'ê°„', 'í›„', 'ì „', 'ë‚´', 'ì™¸',
    'ìƒ', 'í•˜', 'ì¢Œ', 'ìš°', 'ì•', 'ë’¤', 'ìœ„', 'ì•„ë˜', 'ì‚¬ì´', 'ì†', 'ë°–', 'ì•ˆ', 'ì—¬ëŸ¬', 'ê°ì¢…',
    'í•˜ë‚˜', 'ë‘˜', 'ì…‹', 'ìˆëŠ”', 'ì—†ëŠ”', 'ë˜ëŠ”', 'í•˜ëŠ”', 'í°', 'ì‘ì€', 'ìƒˆë¡œìš´', 'ì˜¤ë˜ëœ',
    'ê·¸ëŸ°ë°', 'ê·¸ë˜ì„œ', 'ë˜', 'ë˜í•œ', 'ì—­ì‹œ', 'ë¬¼ë¡ ', 'ë‹¹ì—°íˆ', 'í™•ì‹¤íˆ', 'ì•„ë§ˆ', 'ì •ë§',
    'ë„ˆë¬´', 'ë§¤ìš°', 'ìƒë‹¹íˆ', 'ê½¤', 'ì¡°ê¸ˆ', 'ì•½ê°„', 'ì‚´ì§', 'ì¢€', 'ì ê¹', 'í•œë²ˆ', 'ë‘ë²ˆ',
    'ì²˜ìŒ', 'ë§ˆì§€ë§‰', 'ë‹¤ìŒ', 'ì´ì „', 'ê³„ì†', 'í•­ìƒ', 'ê°€ë”', 'ìì£¼', 'ë•Œë•Œë¡œ', 'ë³´í†µ',
    'ì¼ë°˜ì ', 'íŠ¹ë³„í•œ', 'ì¤‘ìš”í•œ', 'í•„ìš”í•œ', 'ê°€ëŠ¥í•œ', 'ì–´ë ¤ìš´', 'ì‰¬ìš´', 'ë³µì¡í•œ', 'ê°„ë‹¨í•œ'
}

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¹€)
    text = re.sub(r'[^ê°€-í£0-9\s]', ' ', text)
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_keywords_simple(text, min_length=2, max_keywords=20):
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜)"""
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    cleaned_text = clean_text(text)
    
    # ë‹¨ì–´ ë¶„ë¦¬ (ê³µë°± ê¸°ì¤€)
    words = cleaned_text.split()
    
    # í•„í„°ë§: ê¸¸ì´, ë¶ˆìš©ì–´, ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ë‹¨ì–´ ì œì™¸
    filtered_words = []
    for word in words:
        if (len(word) >= min_length and 
            word not in KOREAN_STOPWORDS and 
            not word.isdigit() and
            re.search(r'[ê°€-í£]', word)):  # í•œê¸€ì´ í¬í•¨ëœ ë‹¨ì–´ë§Œ
            filtered_words.append(word)
    
    # ë¹ˆë„ ê³„ì‚°
    word_counts = Counter(filtered_words)
    
    # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
    top_keywords = word_counts.most_common(max_keywords)
    
    # ê°€ì¤‘ì¹˜ ê³„ì‚° (ìµœëŒ€ê°’ì„ 10ìœ¼ë¡œ ì •ê·œí™”)
    if not top_keywords:
        return {}
    
    max_count = top_keywords[0][1]
    weighted_keywords = {}
    
    for word, count in top_keywords:
        # 1~10 ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ë¡œ ê³„ì‚°
        weight = max(1, round((count / max_count) * 10))
        weighted_keywords[word] = weight
    
    return weighted_keywords

def extract_keywords_ngram(text, min_length=2, max_keywords=20):
    """N-gram ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    cleaned_text = clean_text(text)
    
    # ë‹¨ì–´ì™€ 2-gram, 3-gram ì¶”ì¶œ
    words = cleaned_text.split()
    
    # 1-gram (ë‹¨ì–´)
    unigrams = []
    for word in words:
        if (len(word) >= min_length and 
            word not in KOREAN_STOPWORDS and 
            not word.isdigit() and
            re.search(r'[ê°€-í£]', word)):
            unigrams.append(word)
    
    # 2-gram
    bigrams = []
    for i in range(len(words) - 1):
        bigram = words[i] + ' ' + words[i + 1]
        if (len(bigram) >= 4 and 
            words[i] not in KOREAN_STOPWORDS and 
            words[i + 1] not in KOREAN_STOPWORDS and
            re.search(r'[ê°€-í£]', bigram)):
            bigrams.append(bigram)
    
    # ë¹ˆë„ ê³„ì‚°
    all_terms = unigrams + bigrams
    term_counts = Counter(all_terms)
    
    # TF-IDF ìŠ¤íƒ€ì¼ì˜ ê°€ì¤‘ì¹˜ ì ìš© (ê°„ë‹¨ ë²„ì „)
    weighted_terms = {}
    total_terms = len(all_terms)
    
    for term, count in term_counts.items():
        # TF (ìš©ì–´ ë¹ˆë„)
        tf = count / total_terms
        # ê°„ë‹¨í•œ ê°€ì¤‘ì¹˜ (ë¹ˆë„ì™€ ê¸¸ì´ ê³ ë ¤)
        length_bonus = min(2.0, len(term.split()) * 0.5 + 1)
        score = tf * length_bonus * 1000  # ìŠ¤ì¼€ì¼ë§
        weighted_terms[term] = score
    
    # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
    top_terms = sorted(weighted_terms.items(), key=lambda x: x[1], reverse=True)[:max_keywords]
    
    # 1~10 ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ë¡œ ì •ê·œí™”
    if not top_terms:
        return {}
    
    max_score = top_terms[0][1]
    final_keywords = {}
    
    for term, score in top_terms:
        weight = max(1, round((score / max_score) * 10))
        final_keywords[term] = weight
    
    return final_keywords

def format_keywords_output(keywords_dict):
    """í‚¤ì›Œë“œë¥¼ ìš”ì²­í•œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥"""
    if not keywords_dict:
        return "í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ê°€ì¤‘ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_keywords = sorted(keywords_dict.items(), key=lambda x: x[1], reverse=True)
    
    # "í‚¤ì›Œë“œA 5, í‚¤ì›Œë“œB 4, í‚¤ì›Œë“œC 3" í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    formatted_parts = []
    for keyword, weight in sorted_keywords:
        formatted_parts.append(f"{keyword} {weight}")
    
    return ", ".join(formatted_parts)

def create_wordcloud_from_keywords(keywords_dict, width=800, height=600):
    """í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬ë¡œë¶€í„° ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    if not keywords_dict:
        return None
    
    # í°íŠ¸ ê²½ë¡œ í™•ì¸
    font_path = "./fonts/NanumGothic-Regular.ttf"
    if not os.path.exists(font_path):
        st.error(f"í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}")
        st.info("í°íŠ¸ íŒŒì¼ì´ ./fonts/NanumGothic-Regular.ttf ê²½ë¡œì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    
    try:
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        wc = WordCloud(
            font_path=font_path,
            width=width,
            height=height,
            background_color='white',
            max_words=100,
            relative_scaling=0.5,
            min_font_size=10,
            colormap='viridis'
        ).generate_from_frequencies(keywords_dict)
        
        return wc
    except Exception as e:
        st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None

def wordcloud_to_image(wc, width=800, height=600):
    """ì›Œë“œí´ë¼ìš°ë“œë¥¼ PIL Imageë¡œ ë³€í™˜"""
    fig, ax = plt.subplots(figsize=(width/100, height/100))
    ax.imshow(wc, interpolation='bilinear')
    ax.axis('off')
    
    buf = io.BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0, dpi=100)
    buf.seek(0)
    plt.close(fig)
    
    image = Image.open(buf)
    return image

def get_image_download_link(img, filename):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±"""
    buffered = io.BytesIO()
    img.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    href = f'<a href="data:file/png;base64,{img_str}" download="{filename}">ğŸ“¥ ì›Œë“œí´ë¼ìš°ë“œ ë‹¤ìš´ë¡œë“œ</a>'
    return href

# ì•± ì œëª© ë° ì„¤ëª…
st.title("ğŸ” í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ & ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°")
st.markdown("í•œêµ­ì–´ ë¬¸ë‹¨ì„ ì…ë ¥í•˜ë©´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•´ë“œë¦½ë‹ˆë‹¤!")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("âš™ï¸ ì¶”ì¶œ ì„¤ì •")

extraction_method = st.sidebar.selectbox(
    "ì¶”ì¶œ ë°©ë²•",
    ["ë‹¨ìˆœ ë¹ˆë„ ê¸°ë°˜", "N-gram ê¸°ë°˜ (ì¶”ì²œ)"],
    index=1
)

min_word_length = st.sidebar.slider("ìµœì†Œ ë‹¨ì–´ ê¸¸ì´", 1, 5, 2)
max_keywords = st.sidebar.slider("ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜", 10, 50, 20)

# ì›Œë“œí´ë¼ìš°ë“œ ì„¤ì •
st.sidebar.header("ğŸ¨ ì›Œë“œí´ë¼ìš°ë“œ ì„¤ì •")
wc_width = st.sidebar.slider("ë„ˆë¹„", 400, 1200, 800)
wc_height = st.sidebar.slider("ë†’ì´", 300, 800, 600)

# ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
st.header("ğŸ“ ë¬¸ë‹¨ ì…ë ¥")

# ì‚¬ìš©ë²• ì•ˆë‚´
with st.expander("ğŸ’¡ ì‚¬ìš©ë²• ë° ì˜ˆì‹œ"):
    st.markdown("""
    **ì‚¬ìš©ë²•:**
    1. ì•„ë˜ í…ìŠ¤íŠ¸ ìƒìì— ë¶„ì„í•˜ê³  ì‹¶ì€ í•œêµ­ì–´ ë¬¸ë‹¨ì„ ì…ë ¥í•˜ì„¸ìš”
    2. 'í‚¤ì›Œë“œ ì¶”ì¶œ' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
    3. ì¶”ì¶œëœ í‚¤ì›Œë“œì™€ ê°€ì¤‘ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”
    4. ì›Œë“œí´ë¼ìš°ë“œë„ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤
    
    **ì˜ˆì‹œ í…ìŠ¤íŠ¸:**
    ```
    êµìœ¡ì€ ë¯¸ë˜ë¥¼ ì¤€ë¹„í•˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ê³¼ì •ì…ë‹ˆë‹¤. í•™ìƒë“¤ì€ ì°½ì˜ì  ì‚¬ê³ ì™€ 
    ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ ê¸°ë¥´ê¸° ìœ„í•´ ë‹¤ì–‘í•œ í•™ìŠµ ê²½í—˜ì´ í•„ìš”í•©ë‹ˆë‹¤. 
    í˜‘ë ¥ í•™ìŠµì„ í†µí•´ ì†Œí†µ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ë””ì§€í„¸ ê¸°ìˆ ì„ í™œìš©í•œ 
    í˜ì‹ ì ì¸ êµìœ¡ ë°©ë²•ìœ¼ë¡œ í•™ìŠµ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ```
    
    **ì¶œë ¥ í˜•ì‹:**
    í‚¤ì›Œë“œA 5, í‚¤ì›Œë“œB 4, í‚¤ì›Œë“œC 3
    """)

# í…ìŠ¤íŠ¸ ì…ë ¥ ì˜ì—­
text_input = st.text_area(
    "ë¶„ì„í•  í•œêµ­ì–´ ë¬¸ë‹¨ì„ ì…ë ¥í•˜ì„¸ìš”",
    height=200,
    placeholder="ì˜ˆì‹œ: êµìœ¡ì€ ë¯¸ë˜ë¥¼ ì¤€ë¹„í•˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ê³¼ì •ì…ë‹ˆë‹¤. í•™ìƒë“¤ì€ ì°½ì˜ì  ì‚¬ê³ ì™€ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ ê¸°ë¥´ê¸° ìœ„í•´...",
    help="í•œêµ­ì–´ ë¬¸ë‹¨ì„ ì…ë ¥í•˜ë©´ í‚¤ì›Œë“œë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤."
)

# í‚¤ì›Œë“œ ì¶”ì¶œ ë²„íŠ¼
if st.button("ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ", type="primary"):
    if text_input.strip():
        with st.spinner("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            if extraction_method == "ë‹¨ìˆœ ë¹ˆë„ ê¸°ë°˜":
                keywords_dict = extract_keywords_simple(text_input, min_word_length, max_keywords)
            else:
                keywords_dict = extract_keywords_ngram(text_input, min_word_length, max_keywords)
            
            if keywords_dict:
                st.success(f"ì´ {len(keywords_dict)}ê°œì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤!")
                
                # ê²°ê³¼ í‘œì‹œ
                st.header("ğŸ“Š ì¶”ì¶œ ê²°ê³¼")
                
                # ìš”ì²­ëœ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
                formatted_output = format_keywords_output(keywords_dict)
                st.subheader("ğŸ¯ í‚¤ì›Œë“œ ë° ê°€ì¤‘ì¹˜")
                st.code(formatted_output, language=None)
                
                # ë³µì‚¬ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë°•ìŠ¤
                st.text_area(
                    "ë³µì‚¬ìš© ê²°ê³¼", 
                    value=formatted_output, 
                    height=100,
                    help="ì´ í…ìŠ¤íŠ¸ë¥¼ ë³µì‚¬í•´ì„œ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
                
                # ìƒì„¸ í‚¤ì›Œë“œ ì •ë³´
                with st.expander("ğŸ” ìƒì„¸ í‚¤ì›Œë“œ ì •ë³´"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("**í‚¤ì›Œë“œ**")
                        for keyword in keywords_dict.keys():
                            st.write(f"â€¢ {keyword}")
                    with col2:
                        st.write("**ê°€ì¤‘ì¹˜**")
                        for weight in keywords_dict.values():
                            st.write(f"â€¢ {weight}")
                
                # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
                st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
                
                with st.spinner("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    wordcloud = create_wordcloud_from_keywords(keywords_dict, wc_width, wc_height)
                    
                    if wordcloud:
                        # ì›Œë“œí´ë¼ìš°ë“œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                        img = wordcloud_to_image(wordcloud, wc_width, wc_height)
                        
                        # ì´ë¯¸ì§€ í‘œì‹œ
                        st.image(img, caption="ìƒì„±ëœ ì›Œë“œí´ë¼ìš°ë“œ", use_column_width=True)
                        
                        # ë‹¤ìš´ë¡œë“œ ë§í¬
                        download_link = get_image_download_link(img, "keyword_wordcloud.png")
                        st.markdown(download_link, unsafe_allow_html=True)
                        
                        # ìƒì„± ì •ë³´
                        st.info(f"í¬ê¸°: {wc_width}x{wc_height} | ì¶”ì¶œ ë°©ë²•: {extraction_method} | í‚¤ì›Œë“œ ìˆ˜: {len(keywords_dict)}ê°œ")
                
            else:
                st.error("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        st.warning("ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# í‘¸í„°
st.markdown("---")
st.markdown("ğŸ’¡ **íŒ**: ë” ì •í™•í•œ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•´ ì¶©ë¶„íˆ ê¸´ ë¬¸ë‹¨ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
st.markdown("ğŸ“ êµìœ¡ìš© í‚¤ì›Œë“œ ì¶”ì¶œê¸° | Made with Streamlit")