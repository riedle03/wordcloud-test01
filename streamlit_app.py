# GPT API í‚¤ì›Œë“œ ì¶”ì¶œ & ì›Œë“œí´ë¼ìš°ë“œ + ë„¤íŠ¸ì›Œí¬ ë¶„ì„ Streamlit ì•± (í•œê¸€ ê¸€ê¼´ ê¹¨ì§ ìˆ˜ì • ë²„ì „)
# -----------------------------------------------------------------------------
# ì£¼ìš” ë³€ê²½ì : draw_network_graph() í•¨ìˆ˜ì—ì„œ font_family ë§¤ê°œë³€ìˆ˜ë¥¼ ì œê±°í•˜ì—¬ 
#            ì „ì—­ rcParams ì„¤ì •(í•œê¸€ í°íŠ¸)ì´ ê·¸ëŒ€ë¡œ ì ìš©ë˜ë„ë¡ ìˆ˜ì •.
# -----------------------------------------------------------------------------

import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from wordcloud import WordCloud
import io
import base64
from PIL import Image
import numpy as np
import os
import re
import requests
import networkx as nx
from collections import Counter
import itertools

# =========================================
# 1. í•œê¸€ í°íŠ¸ ì„¤ì •
# =========================================
# Pretendard TTF ê²½ë¡œ
FONT_PATH = "./fonts/Pretendard-Bold.ttf"

# FontProperties ìƒì„±
pretendard_prop = fm.FontProperties(fname=FONT_PATH)
font_name = pretendard_prop.get_name()  # ì‹¤ì œ ë‚´ë¶€ í°íŠ¸ ì´ë¦„ ("Pretendard")

# rcParamsì—ë„ ë“±ë¡
plt.rcParams["font.family"] = font_name
plt.rcParams["axes.unicode_minus"] = False



def setup_korean_font() -> bool:
    """Pretendard í°íŠ¸ë¥¼ matplotlib ê¸°ë³¸ í°íŠ¸ë¡œ ë“±ë¡í•œë‹¤."""
    if os.path.exists(FONT_PATH):
        font_prop = fm.FontProperties(fname=FONT_PATH)
        plt.rcParams["font.family"] = font_prop.get_name()
        plt.rcParams["axes.unicode_minus"] = False  # í•œê¸€ í°íŠ¸ ì‚¬ìš© ì‹œ â€“ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
        return True
    st.warning(f"í•œê¸€ í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FONT_PATH}")
    st.info("`/fonts/Pretendard-Bold.ttf` ê²½ë¡œì— í°íŠ¸ íŒŒì¼ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")
    return False


setup_korean_font()

# =========================================
# 2. Streamlit í˜ì´ì§€ ì„¤ì •
# =========================================
st.set_page_config(
    page_title="GPT í‚¤ì›Œë“œ ì¶”ì¶œ & ì›Œë“œí´ë¼ìš°ë“œ",
    page_icon="ğŸ¤–",
    layout="wide",
)

# =========================================
# 3. OpenAI API í˜¸ì¶œ
# =========================================

def call_openai_api(text: str, api_key: str, model: str = "gpt-4o-mini") -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ë„ë¡ OpenAI ChatCompletion í˜¸ì¶œ"""
    prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•˜ê³  ì¤‘ìš”ë„ì— ë”°ë¼ 1~10 ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text}"

ìš”êµ¬ì‚¬í•­:
1. 10~20ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
2. ê° í‚¤ì›Œë“œì— 1~10 ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ ë¶€ì—¬ (10ì´ ê°€ì¥ ì¤‘ìš”)
3. ë¶ˆìš©ì–´(ì¡°ì‚¬, ì–´ë¯¸, ì¼ë°˜ì ì¸ ë‹¨ì–´) ì œì™¸
4. ë³µí•©ì–´ë‚˜ ì¤‘ìš”í•œ êµ¬ë¬¸ë„ í¬í•¨ ê°€ëŠ¥
5. ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:

í‚¤ì›Œë“œA 5, í‚¤ì›Œë“œB 4, í‚¤ì›Œë“œC 3

ìœ„ í˜•ì‹ ì™¸ì˜ ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
"""

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    data = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 500,
        "temperature": 0.3,
    }

    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=30,
        )
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"].strip()
        return f"API ì˜¤ë¥˜: {response.status_code} - {response.text}"
    except requests.exceptions.RequestException as e:
        return f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}"
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {e}"

# =========================================
# 4. GPT ì‘ë‹µ íŒŒì‹±
# =========================================

def parse_gpt_response(response_text: str) -> dict[str, int]:
    """'í‚¤ì›Œë“œ 5, í‚¤ì›Œë“œ2 4, ...' í˜•ì‹ì„ dict ë¡œ ë³€í™˜"""
    keywords: dict[str, int] = {}
    for item in response_text.split(","):
        item = item.strip()
        if not item:
            continue
        parts = item.rsplit(" ", 1)
        if len(parts) == 2 and parts[1].isdigit():
            keywords[parts[0].strip()] = int(parts[1])
        else:
            keywords[item] = 5  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
    return keywords

# =========================================
# 5. ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
# =========================================

def create_wordcloud_from_keywords(keywords: dict[str, int], width=800, height=600, bg_color="white"):
    if not keywords:
        return None
    if not os.path.exists(FONT_PATH):
        st.error("í°íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    try:
        wc = WordCloud(
            font_path=FONT_PATH,
            width=width,
            height=height,
            background_color=bg_color,
            max_words=100,
            relative_scaling=0.5,
            prefer_horizontal=0.7,
        ).generate_from_frequencies(keywords)
        return wc
    except Exception as e:
        st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
        return None


def wordcloud_to_image(wc, width=800, height=600):
    fig, ax = plt.subplots(figsize=(width / 100, height / 100))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    buf = io.BytesIO()
    plt.savefig(buf, format="png", dpi=100, bbox_inches="tight", pad_inches=0)
    buf.seek(0)
    plt.close(fig)
    return Image.open(buf)

# =========================================
# 6. ë„¤íŠ¸ì›Œí¬ ë¶„ì„
# =========================================

def create_network_analysis(text: str, keywords: dict[str, int], min_cooccurrence: int = 1):
    if len(keywords) < 2:
        return None, None

    sentences = re.split(r"[.!?]\s*", text)
    co_occurrence = Counter()
    keys = list(keywords.keys())

    for sentence in sentences:
        found = [kw for kw in keys if kw in sentence]
        for a, b in itertools.combinations(found, 2):
            co_occurrence[tuple(sorted((a, b)))] += 1

    filtered = {k: v for k, v in co_occurrence.items() if v >= min_cooccurrence}
    if not filtered:
        return None, None

    G = nx.Graph()
    nodes = set(itertools.chain.from_iterable(filtered.keys()))
    for n in nodes:
        G.add_node(n, weight=keywords.get(n, 1))
    for (a, b), w in filtered.items():
        G.add_edge(a, b, weight=w)

    return G, filtered


# -------------------
# draw_network_graph (FIX)
# -------------------

def draw_network_graph(G: nx.Graph, keywords: dict[str, int]):
    if G is None or len(G.nodes()) < 2:
        return None

    fig, ax = plt.subplots(figsize=(12, 8))
    pos = nx.spring_layout(G, k=0.8 / np.sqrt(G.number_of_nodes()), iterations=50)

    node_sizes = [keywords.get(n, 1) * 300 for n in G.nodes()]
    edge_widths = [G[u][v]["weight"] * 1.5 for u, v in G.edges()]

    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color="lightblue", alpha=0.7, edgecolors="darkblue", linewidths=1.5, ax=ax)
    nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.6, edge_color="gray", ax=ax)

    # â˜… font_family ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ (rcParams ê°’ì´ ê·¸ëŒ€ë¡œ ì ìš©!)
    nx.draw_networkx_labels(
        G,
        pos,
        labels={n: n for n in G.nodes()},
        font_size=9,
        font_color="black",
        font_weight="bold",
        ax=ax,
        font_family=font_name  # ì—¬ê¸°ì„œ ì •í™•í•œ ë‚´ë¶€ í°íŠ¸ ì´ë¦„ì„ ì¤˜ì•¼ í•¨
    )

    ax.set_title("í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„", fontsize=16, fontweight="bold")
    ax.axis("off")
    plt.tight_layout()

    buf = io.BytesIO()
    plt.savefig(buf, format="png", dpi=300, bbox_inches="tight")
    buf.seek(0)
    img = Image.open(buf)
    plt.close(fig)
    return img

# =========================================
# 7. ë„¤íŠ¸ì›Œí¬ ì§€í‘œ ë¶„ì„ (optional)
# =========================================

def analyze_network_metrics(G: nx.Graph):
    if G is None or len(G.nodes()) < 2:
        return {}
    metrics = {
        "nodes": G.number_of_nodes(),
        "edges": G.number_of_edges(),
        "density": nx.density(G) if nx.is_connected(G) else "N/A",
        "components": nx.number_connected_components(G),
    }
    deg = nx.degree_centrality(G)
    bt = nx.betweenness_centrality(G)
    cl = nx.closeness_centrality(G)
    metrics["degree_top5"] = dict(sorted(deg.items(), key=lambda x: x[1], reverse=True)[:5])
    metrics["betweenness_top5"] = dict(sorted(bt.items(), key=lambda x: x[1], reverse=True)[:5])
    metrics["closeness_top5"] = dict(sorted(cl.items(), key=lambda x: x[1], reverse=True)[:5])
    return metrics

# =========================================
# 8. Streamlit UI
# =========================================

st.title("ğŸ¤– GPT API í‚¤ì›Œë“œ ì¶”ì¶œ & ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°")
st.markdown("GPTë¥¼ í™œìš©í•´ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤!")

st.sidebar.header("ğŸ”‘ API ì„¤ì •")
api_key = st.sidebar.text_input("OpenAI API Key", type="password")
model_choice = st.sidebar.selectbox("GPT ëª¨ë¸", ["gpt-4o-mini", "gpt-3.5-turbo"], index=0)

st.sidebar.header("ğŸ¨ ì‹œê°í™” ì„¤ì •")
wc_width = st.sidebar.slider("ì›Œë“œí´ë¼ìš°ë“œ ë„ˆë¹„", 400, 1200, 800)
wc_height = st.sidebar.slider("ì›Œë“œí´ë¼ìš°ë“œ ë†’ì´", 300, 800, 600)
bg_color = st.sidebar.selectbox("ë°°ê²½ìƒ‰", ["white", "black"], index=0)

show_network = st.sidebar.checkbox("ğŸŒ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í¬í•¨", value=True)
min_co = st.sidebar.slider("ìµœì†Œ ë™ì‹œì¶œí˜„ íšŸìˆ˜", 1, 5, 1)

st.header("ğŸ“ í…ìŠ¤íŠ¸ ì…ë ¥")
text_input = st.text_area("ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", height=200)

if st.button("ğŸ¤– GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ", disabled=not api_key):
    if not text_input.strip():
        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("GPT í˜¸ì¶œ ì¤‘..."):
            response = call_openai_api(text_input, api_key, model_choice)
        if response.startswith("API ì˜¤ë¥˜") or response.startswith("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜"):
            st.error(response)
        else:
            st.success("í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
            st.subheader("ğŸ“œ GPT ì‘ë‹µ")
            st.code(response)

            keywords = parse_gpt_response(response)
            if not keywords:
                st.error("ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨")
            else:
                # í‚¤ì›Œë“œ ëª©ë¡ í‘œì‹œ
                st.subheader("ğŸ“Š ì¶”ì¶œëœ í‚¤ì›Œë“œ")
                for k, v in sorted(keywords.items(), key=lambda x: x[1], reverse=True):
                    st.write(f"- **{k}**: {v}")

                # ì›Œë“œí´ë¼ìš°ë“œ
                st.subheader("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
                wc = create_wordcloud_from_keywords(keywords, wc_width, wc_height, bg_color)
                if wc:
                    img_wc = wordcloud_to_image(wc, wc_width, wc_height)
                    st.image(img_wc, caption="WordCloud", use_column_width=True)

                # ë„¤íŠ¸ì›Œí¬ ë¶„ì„
                if show_network:
                    st.subheader("ğŸŒ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬")
                    G, co = create_network_analysis(text_input, keywords, min_co)
                    if G:
                        img_net = draw_network_graph(G, keywords)
                        st.image(img_net, caption="Keyword Network", use_column_width=True)
                        # ì§€í‘œ
                        metrics = analyze_network_metrics(G)
                        st.markdown("### ë„¤íŠ¸ì›Œí¬ ì§€í‘œ")
                        st.json(metrics)
                    else:
                        st.info("ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì„±í•  ì¶©ë¶„í•œ ë™ì‹œì¶œí˜„ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
