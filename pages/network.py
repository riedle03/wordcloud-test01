import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from wordcloud import WordCloud
import io
import base64
from PIL import Image
import numpy as np
import os
import re
import requests
import json
import networkx as nx
from collections import Counter
import itertools

# í•œê¸€ í°íŠ¸ ì„¤ì •
def setup_korean_font():
    """í•œê¸€ í°íŠ¸ë¥¼ matplotlibì— ì„¤ì •"""
    font_path = "./fonts/Pretendard-Bold.ttf"
    if os.path.exists(font_path):
        # í°íŠ¸ ë“±ë¡
        font_prop = fm.FontProperties(fname=font_path)
        plt.rcParams['font.family'] = font_prop.get_name()
        plt.rcParams['font.size'] = 10
        plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
        return True
    else:
        st.warning("í•œê¸€ í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return False

# ì•± ì‹œì‘ ì‹œ í°íŠ¸ ì„¤ì •
setup_korean_font()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="GPT API í‚¤ì›Œë“œ ì¶”ì¶œ & ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°",
    page_icon="ğŸ¤–",
    layout="wide"
)

def call_openai_api(text, api_key, model="gpt-4.1-mini"):
    """OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    
    prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•˜ê³  ì¤‘ìš”ë„ì— ë”°ë¼ 1~10 ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text}"

ìš”êµ¬ì‚¬í•­:
1. 10~20ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
2. ê° í‚¤ì›Œë“œì— 1~10 ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ ë¶€ì—¬ (10ì´ ê°€ì¥ ì¤‘ìš”)
3. ë¶ˆìš©ì–´(ì¡°ì‚¬, ì–´ë¯¸, ì¼ë°˜ì ì¸ ë‹¨ì–´) ì œì™¸
4. ë³µí•©ì–´ë‚˜ ì¤‘ìš”í•œ êµ¬ë¬¸ë„ í¬í•¨ ê°€ëŠ¥
5. ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:

í‚¤ì›Œë“œA 5, í‚¤ì›Œë“œB 4, í‚¤ì›Œë“œC 3

ìœ„ í˜•ì‹ ì™¸ì˜ ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
"""

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [
            {
                "role": "user", 
                "content": prompt
            }
        ],
        "max_tokens": 500,
        "temperature": 0.3
    }
    
    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
        else:
            return f"API ì˜¤ë¥˜: {response.status_code} - {response.text}"
            
    except requests.exceptions.RequestException as e:
        return f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}"
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def parse_gpt_response(response_text):
    """GPT ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
    keywords_dict = {}
    
    try:
        # ì‰¼í‘œë¡œ ë¶„ë¦¬
        items = response_text.split(',')
        
        for item in items:
            item = item.strip()
            # ë§ˆì§€ë§‰ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ ìˆ«ìë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì²˜ë¦¬
            parts = item.rsplit(' ', 1)
            
            if len(parts) == 2:
                keyword = parts[0].strip()
                try:
                    weight = int(parts[1].strip())
                    if 1 <= weight <= 10:  # ê°€ì¤‘ì¹˜ ë²”ìœ„ ê²€ì¦
                        keywords_dict[keyword] = weight
                except ValueError:
                    # ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ì „ì²´ë¥¼ í‚¤ì›Œë“œë¡œ ì²˜ë¦¬
                    keywords_dict[item] = 5
            else:
                # ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ 5
                keywords_dict[item] = 5
                
        return keywords_dict
        
    except Exception as e:
        st.error(f"ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        return {}

def create_wordcloud_from_keywords(keywords_dict, width=800, height=600, bg_color='white'):
    """í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬ë¡œë¶€í„° ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    if not keywords_dict:
        return None
    
    # í°íŠ¸ ê²½ë¡œ í™•ì¸
    font_path = "./fonts/Pretendard-Bold.ttf"
    if not os.path.exists(font_path):
        st.error(f"í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}")
        st.info("í°íŠ¸ íŒŒì¼ì´ ./fonts/Pretendard-Bold.ttf ê²½ë¡œì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    
    try:
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        wc = WordCloud(
            font_path=font_path,
            width=width,
            height=height,
            background_color=bg_color,
            max_words=100,
            relative_scaling=0.5,
            min_font_size=10,
            colormap='viridis',
            prefer_horizontal=0.7
        ).generate_from_frequencies(keywords_dict)
        
        return wc
    except Exception as e:
        st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None

def wordcloud_to_image(wc, width=800, height=600):
    """ì›Œë“œí´ë¼ìš°ë“œë¥¼ PIL Imageë¡œ ë³€í™˜"""
    fig, ax = plt.subplots(figsize=(width/100, height/100))
    ax.imshow(wc, interpolation='bilinear')
    ax.axis('off')
    
    buf = io.BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0, dpi=100)
    buf.seek(0)
    plt.close(fig)
    
    image = Image.open(buf)
    return image

def create_network_analysis(text, keywords_dict):
    """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ê°„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
    if not keywords_dict or len(keywords_dict) < 2:
        return None, None
    
    # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
    sentences = re.split(r'[.!?]\s*', text)
    
    # í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
    co_occurrence = Counter()
    keyword_list = list(keywords_dict.keys())
    
    for sentence in sentences:
        # ë¬¸ì¥ì— í¬í•¨ëœ í‚¤ì›Œë“œë“¤ ì°¾ê¸°
        found_keywords = []
        for keyword in keyword_list:
            if keyword in sentence:
                found_keywords.append(keyword)
        
        # ê°™ì€ ë¬¸ì¥ì— ë‚˜íƒ€ë‚œ í‚¤ì›Œë“œë“¤ì˜ ì¡°í•© ìƒì„±
        for combo in itertools.combinations(found_keywords, 2):
            # ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ (A,B)ì™€ (B,A)ë¥¼ ê°™ê²Œ ì²˜ë¦¬
            sorted_combo = tuple(sorted(combo))
            co_occurrence[sorted_combo] += 1
    
    if not co_occurrence:
        return None, None
    
    # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
    G = nx.Graph()
    
    # í‚¤ì›Œë“œë¥¼ ë…¸ë“œë¡œ ì¶”ê°€ (ê°€ì¤‘ì¹˜ë¥¼ ë…¸ë“œ í¬ê¸°ë¡œ ì‚¬ìš©)
    for keyword, weight in keywords_dict.items():
        G.add_node(keyword, weight=weight)
    
    # ë™ì‹œì¶œí˜„ì„ ì—£ì§€ë¡œ ì¶”ê°€
    for (keyword1, keyword2), freq in co_occurrence.items():
        if freq > 0:  # ì„ê³„ê°’ ì„¤ì • ê°€ëŠ¥
            G.add_edge(keyword1, keyword2, weight=freq)
    
    return G, co_occurrence

def draw_network_graph(G, keywords_dict):
    """ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°"""
    if G is None or len(G.nodes()) < 2:
        return None
    
    # í•œê¸€ í°íŠ¸ ì„¤ì •
    font_path = "./fonts/Pretendard-Bold.ttf"
    font_prop = None
    if os.path.exists(font_path):
        font_prop = fm.FontProperties(fname=font_path)
    
    # ê·¸ë˜í”„ ë ˆì´ì•„ì›ƒ ì„¤ì •
    plt.figure(figsize=(12, 8))
    
    # ìŠ¤í”„ë§ ë ˆì´ì•„ì›ƒ ì‚¬ìš©
    pos = nx.spring_layout(G, k=3, iterations=50)
    
    # ë…¸ë“œ í¬ê¸° ì„¤ì • (í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ê¸°ë°˜)
    node_sizes = [keywords_dict.get(node, 1) * 300 for node in G.nodes()]
    
    # ì—£ì§€ ë‘ê»˜ ì„¤ì • (ë™ì‹œì¶œí˜„ ë¹ˆë„ ê¸°ë°˜)
    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
    edge_widths = [w * 2 for w in edge_weights]
    
    # ë…¸ë“œ ê·¸ë¦¬ê¸°
    nx.draw_networkx_nodes(G, pos, 
                          node_size=node_sizes, 
                          node_color='lightblue', 
                          alpha=0.7,
                          edgecolors='darkblue',
                          linewidths=2)
    
    # ì—£ì§€ ê·¸ë¦¬ê¸°
    nx.draw_networkx_edges(G, pos, 
                          width=edge_widths, 
                          alpha=0.6, 
                          edge_color='gray')
    
    # ë¼ë²¨ ê·¸ë¦¬ê¸°
    if font_prop:
        nx.draw_networkx_labels(G, pos, 
                               font_size=10, 
                               font_color='black',
                               font_weight='bold',
                               fontproperties=font_prop)
    else:
        nx.draw_networkx_labels(G, pos, 
                               font_size=10, 
                               font_color='black',
                               font_weight='bold')
    
    plt.title('í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„', 
              fontproperties=font_prop if font_prop else None, 
              fontsize=16, 
              fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    
    # ì´ë¯¸ì§€ë¡œ ë³€í™˜
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')
    buf.seek(0)
    image = Image.open(buf)
    plt.close()
    
    return image

def analyze_network_metrics(G, keywords_dict):
    """ë„¤íŠ¸ì›Œí¬ ì§€í‘œ ë¶„ì„"""
    if G is None or len(G.nodes()) < 2:
        return {}
    
    metrics = {}
    
    # ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ ì •ë³´
    metrics['nodes'] = G.number_of_nodes()
    metrics['edges'] = G.number_of_edges()
    metrics['density'] = nx.density(G)
    
    # ì¤‘ì‹¬ì„± ì§€í‘œ ê³„ì‚°
    try:
        degree_centrality = nx.degree_centrality(G)
        betweenness_centrality = nx.betweenness_centrality(G)
        closeness_centrality = nx.closeness_centrality(G)
        
        # ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ ì°¾ê¸°
        metrics['most_connected'] = max(degree_centrality, key=degree_centrality.get)
        metrics['most_between'] = max(betweenness_centrality, key=betweenness_centrality.get)
        metrics['most_close'] = max(closeness_centrality, key=closeness_centrality.get)
        
        # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œì˜ ì¤‘ì‹¬ì„± ì ìˆ˜
        metrics['degree_top5'] = dict(sorted(degree_centrality.items(), 
                                           key=lambda x: x[1], reverse=True)[:5])
        metrics['betweenness_top5'] = dict(sorted(betweenness_centrality.items(), 
                                                key=lambda x: x[1], reverse=True)[:5])
        
    except:
        metrics['most_connected'] = "ê³„ì‚° ë¶ˆê°€"
        metrics['most_between'] = "ê³„ì‚° ë¶ˆê°€"
        metrics['most_close'] = "ê³„ì‚° ë¶ˆê°€"
    
    return metrics
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±"""
    buffered = io.BytesIO()
    img.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    href = f'<a href="data:file/png;base64,{img_str}" download="{filename}">ğŸ“¥ ì›Œë“œí´ë¼ìš°ë“œ ë‹¤ìš´ë¡œë“œ</a>'
    return href

# ì•± ì œëª© ë° ì„¤ëª…
st.title("ğŸ¤– GPT API í‚¤ì›Œë“œ ì¶”ì¶œ & ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°")
st.markdown("GPT APIë¥¼ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤!")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("ğŸ”‘ API ì„¤ì •")

# API í‚¤ ì…ë ¥
api_key = st.sidebar.text_input(
    "OpenAI API Key", 
    type="password",
    help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (https://platform.openai.com/api-keys)"
)

# GPT ëª¨ë¸ ì„ íƒ
model_choice = st.sidebar.selectbox(
    "GPT ëª¨ë¸",
    ["gpt-4.1-mini", "GPT-4.1 nano", "gpt-4o-mini"],
    index=0,
    help="ì‚¬ìš©í•  GPT ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”. gpt-4.1-miniê°€ ê°€ì¥ ê²½ì œì ì´ë©´ì„œë„ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤."
)

# ì›Œë“œí´ë¼ìš°ë“œ ì„¤ì •
st.sidebar.header("ğŸ¨ ì‹œê°í™” ì„¤ì •")
wc_width = st.sidebar.slider("ì›Œë“œí´ë¼ìš°ë“œ ë„ˆë¹„", 400, 1200, 800)
wc_height = st.sidebar.slider("ì›Œë“œí´ë¼ìš°ë“œ ë†’ì´", 300, 800, 600)
bg_color = st.sidebar.selectbox("ë°°ê²½ìƒ‰", ["white", "black"], index=0)

# ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì„¤ì •
show_network = st.sidebar.checkbox("ğŸŒ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í¬í•¨", value=True)
min_cooccurrence = st.sidebar.slider("ìµœì†Œ ë™ì‹œì¶œí˜„ íšŸìˆ˜", 1, 5, 1, 
                                    help="ì´ ê°’ ì´ìƒìœ¼ë¡œ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œë“¤ë§Œ ì—°ê²°ì„ ìœ¼ë¡œ í‘œì‹œ")

# API í‚¤ í™•ì¸
if not api_key:
    st.warning("âš ï¸ OpenAI API í‚¤ë¥¼ ì™¼ìª½ ì‚¬ì´ë“œë°”ì— ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.info("""
    **API í‚¤ ë°œê¸‰ ë°©ë²•:**
    1. https://platform.openai.com ì ‘ì†
    2. ë¡œê·¸ì¸ í›„ 'API Keys' ë©”ë‰´ë¡œ ì´ë™
    3. 'Create new secret key' í´ë¦­
    4. ìƒì„±ëœ í‚¤ë¥¼ ë³µì‚¬í•˜ì—¬ ì‚¬ì´ë“œë°”ì— ì…ë ¥
    
    **ì£¼ì˜ì‚¬í•­:**
    - API ì‚¬ìš©ë£Œê°€ ë¶€ê³¼ë©ë‹ˆë‹¤
    - í‚¤ëŠ” ì•ˆì „í•˜ê²Œ ë³´ê´€í•˜ì„¸ìš”
    
    **í°íŠ¸ íŒŒì¼ ì„¤ì •:**
    - ./fonts/Pretendard-Bold.ttf íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤
    """)

# ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
st.header("ğŸ“ í…ìŠ¤íŠ¸ ì…ë ¥")

# ì‚¬ìš©ë²• ì•ˆë‚´
with st.expander("ğŸ’¡ ì‚¬ìš©ë²• ë° ì˜ˆì‹œ"):
    st.markdown("""
    **ì‚¬ìš©ë²•:**
    1. OpenAI API í‚¤ë¥¼ ì‚¬ì´ë“œë°”ì— ì…ë ¥
    2. ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥
    3. 'GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ' ë²„íŠ¼ í´ë¦­
    4. ì¶”ì¶œëœ í‚¤ì›Œë“œì™€ ì›Œë“œí´ë¼ìš°ë“œ í™•ì¸
    
    **ì˜ˆì‹œ í…ìŠ¤íŠ¸:**
    ```
    ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ ë¶„ì•¼ì…ë‹ˆë‹¤. 
    ë°ì´í„° ê³¼í•™ìë“¤ì€ ë¹…ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ íŒ¨í„´ì„ ì°¾ê³ , 
    ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. 
    ìì—°ì–´ ì²˜ë¦¬ì™€ ì»´í“¨í„° ë¹„ì „ ê¸°ìˆ ì´ ë°œì „í•˜ë©´ì„œ 
    ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì—ì„œ í˜ì‹ ì´ ì¼ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤.
    ```
    
    **GPTì˜ ì¥ì :**
    - ë¬¸ë§¥ì„ ì´í•´í•œ ì •í™•í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
    - ë™ì˜ì–´/ìœ ì˜ì–´ ê·¸ë£¹í•‘
    - ì¤‘ìš”ë„ì— ë”°ë¥¸ ì •êµí•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    - ë³µí•©ì–´ì™€ ì „ë¬¸ìš©ì–´ ì¸ì‹
    """)

# í…ìŠ¤íŠ¸ ì…ë ¥ ì˜ì—­
text_input = st.text_area(
    "ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
    height=200,
    placeholder="ì˜ˆì‹œ: ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ ë¶„ì•¼ì…ë‹ˆë‹¤...",
    help="GPTê°€ ì´ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."
)

# í‚¤ì›Œë“œ ì¶”ì¶œ ë²„íŠ¼
if st.button("ğŸ¤– GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ", type="primary", disabled=not api_key):
    if not api_key:
        st.error("OpenAI API í‚¤ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not text_input.strip():
        st.warning("ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner(f"{model_choice} ëª¨ë¸ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
            # GPT API í˜¸ì¶œ
            gpt_response = call_openai_api(text_input, api_key, model_choice)
            
            if gpt_response.startswith("API ì˜¤ë¥˜") or gpt_response.startswith("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜") or gpt_response.startswith("ì˜¤ë¥˜ ë°œìƒ"):
                st.error(gpt_response)
            else:
                # GPT ì‘ë‹µ í‘œì‹œ
                st.success("í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
                
                st.header("ğŸ¤– GPT ì‘ë‹µ")
                st.code(gpt_response, language=None)
                
                # í‚¤ì›Œë“œ íŒŒì‹±
                keywords_dict = parse_gpt_response(gpt_response)
                
                if keywords_dict:
                    st.header("ğŸ“Š ì¶”ì¶œëœ í‚¤ì›Œë“œ")
                    
                    # í‚¤ì›Œë“œ ì •ë³´ í‘œì‹œ
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.subheader("ğŸ¯ í‚¤ì›Œë“œ ëª©ë¡")
                        for keyword, weight in sorted(keywords_dict.items(), key=lambda x: x[1], reverse=True):
                            st.write(f"**{keyword}**: {weight}")
                    
                    with col2:
                        st.subheader("ğŸ“ˆ ê°€ì¤‘ì¹˜ ë¶„í¬")
                        weights = list(keywords_dict.values())
                        
                        # í•œê¸€ í°íŠ¸ ì¬ì„¤ì • (ì°¨íŠ¸ìš©)
                        font_path = "./fonts/Pretendard-Bold.ttf"
                        if os.path.exists(font_path):
                            font_prop = fm.FontProperties(fname=font_path)
                            
                        fig, ax = plt.subplots(figsize=(6, 4))
                        ax.hist(weights, bins=range(1, 12), alpha=0.7, color='skyblue', edgecolor='black')
                        
                        # í•œê¸€ í°íŠ¸ ì ìš©
                        if os.path.exists(font_path):
                            ax.set_xlabel('ê°€ì¤‘ì¹˜', fontproperties=font_prop)
                            ax.set_ylabel('í‚¤ì›Œë“œ ìˆ˜', fontproperties=font_prop)
                            ax.set_title('í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶„í¬', fontproperties=font_prop)
                        else:
                            ax.set_xlabel('Weight')
                            ax.set_ylabel('Keywords Count')
                            ax.set_title('Keyword Weight Distribution')
                            
                        ax.set_xticks(range(1, 11))
                        ax.grid(True, alpha=0.3)
                        st.pyplot(fig)
                        plt.close()
                    
                    # ë³µì‚¬ ê°€ëŠ¥í•œ ê²°ê³¼
                    st.subheader("ğŸ“‹ ë³µì‚¬ìš© ê²°ê³¼")
                    st.text_area(
                        "ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹", 
                        value=gpt_response, 
                        height=100,
                        help="ì´ í…ìŠ¤íŠ¸ë¥¼ ë³µì‚¬í•´ì„œ ë‹¤ë¥¸ ì›Œë“œí´ë¼ìš°ë“œ ë„êµ¬ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )
                    
                    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
                    st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
                    
                    with st.spinner("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ëŠ” ì¤‘..."):
                        wordcloud = create_wordcloud_from_keywords(keywords_dict, wc_width, wc_height, bg_color)
                        
                        if wordcloud:
                            # ì›Œë“œí´ë¼ìš°ë“œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                            img = wordcloud_to_image(wordcloud, wc_width, wc_height)
                            
                            # ì´ë¯¸ì§€ í‘œì‹œ
                            st.image(img, caption="GPTë¡œ ìƒì„±ëœ ì›Œë“œí´ë¼ìš°ë“œ", use_column_width=True)
                            
                            # ë‹¤ìš´ë¡œë“œ ë§í¬
                            download_link = get_image_download_link(img, "gpt_wordcloud.png")
                            st.markdown(download_link, unsafe_allow_html=True)
                            
                            # ìƒì„± ì •ë³´
                            st.info(f"ëª¨ë¸: {model_choice} | í¬ê¸°: {wc_width}x{wc_height} | í‚¤ì›Œë“œ ìˆ˜: {len(keywords_dict)}ê°œ")
                
                else:
                    st.error("GPT ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ë¹„ìš© ì•ˆë‚´
if api_key:
    st.sidebar.markdown("---")
    st.sidebar.header("ğŸ’° ë¹„ìš© ì•ˆë‚´")
    st.sidebar.markdown("""
    **ì˜ˆìƒ ë¹„ìš© (1íšŒ ìš”ì²­):**
    - gpt-4.1-mini: ~$0.0001-0.0005 (ì¶”ì²œ)
    - GPT-4.1 nano: ~$0.00005-0.0002 (ì´ˆê²½ì œì )
    - gpt-4o-mini: ~$0.0001-0.0005
    
    *gpt-4.1-miniëŠ” ê°€ì¥ ê²½ì œì ì´ë©´ì„œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.*
    """)


# í‘¸í„°
st.markdown("---")
st.markdown("ğŸ’¡ **íŒ**: GPTê°€ ë¬¸ë§¥ì„ ì´í•´í•˜ë¯€ë¡œ ë” ì •í™•í•˜ê³  ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
st.markdown("ğŸ¤– GPT API í‚¤ì›Œë“œ ì¶”ì¶œê¸° | Made with Streamlit")