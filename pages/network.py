import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from wordcloud import WordCloud
import io
import base64
from PIL import Image
import numpy as np
import os
import re
import requests
import json
import networkx as nx
from collections import Counter
import itertools

# í•œê¸€ í°íŠ¸ ì„¤ì •
# í°íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ìƒìˆ˜ë¡œ ì •ì˜í•˜ì—¬ ì¬ì‚¬ìš©ì„±ì„ ë†’ì…ë‹ˆë‹¤.
FONT_PATH = "./fonts/Pretendard-Bold.ttf"

def setup_korean_font():
    """í•œê¸€ í°íŠ¸ë¥¼ matplotlibì— ì„¤ì •"""
    if os.path.exists(FONT_PATH):
        # í°íŠ¸ ë“±ë¡
        font_prop = fm.FontProperties(fname=FONT_PATH)
        plt.rcParams['font.family'] = font_prop.get_name()
        plt.rcParams['font.size'] = 10
        plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
        return True
    else:
        st.warning(f"í•œê¸€ í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {FONT_PATH}")
        st.info("/fonts/Pretendard-Bold.ttf ê²½ë¡œì— í°íŠ¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

# ì•± ì‹œì‘ ì‹œ í°íŠ¸ ì„¤ì •
setup_korean_font()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="GPT API í‚¤ì›Œë“œ ì¶”ì¶œ & ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°",
    page_icon="ğŸ¤–",
    layout="wide"
)

def call_openai_api(text, api_key, model="gpt-4o-mini"): # ëª¨ë¸ ê¸°ë³¸ê°’ì„ ìœ íš¨í•œ ê²ƒìœ¼ë¡œ ë³€ê²½
    """OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    
    prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•˜ê³  ì¤‘ìš”ë„ì— ë”°ë¼ 1~10 ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text}"

ìš”êµ¬ì‚¬í•­:
1. 10~20ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
2. ê° í‚¤ì›Œë“œì— 1~10 ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ ë¶€ì—¬ (10ì´ ê°€ì¥ ì¤‘ìš”)
3. ë¶ˆìš©ì–´(ì¡°ì‚¬, ì–´ë¯¸, ì¼ë°˜ì ì¸ ë‹¨ì–´) ì œì™¸
4. ë³µí•©ì–´ë‚˜ ì¤‘ìš”í•œ êµ¬ë¬¸ë„ í¬í•¨ ê°€ëŠ¥
5. ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:

í‚¤ì›Œë“œA 5, í‚¤ì›Œë“œB 4, í‚¤ì›Œë“œC 3

ìœ„ í˜•ì‹ ì™¸ì˜ ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
"""

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [
            {
                "role": "user", 
                "content": prompt
            }
        ],
        "max_tokens": 500,
        "temperature": 0.3
    }
    
    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
        else:
            return f"API ì˜¤ë¥˜: {response.status_code} - {response.text}"
            
    except requests.exceptions.RequestException as e:
        return f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}"
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def parse_gpt_response(response_text):
    """GPT ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
    keywords_dict = {}
    
    try:
        # ì‰¼í‘œë¡œ ë¶„ë¦¬
        items = response_text.split(',')
        
        for item in items:
            item = item.strip()
            if not item: # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
                continue
            
            # ë§ˆì§€ë§‰ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ ìˆ«ìë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì²˜ë¦¬
            parts = item.rsplit(' ', 1)
            
            if len(parts) == 2:
                keyword = parts[0].strip()
                try:
                    weight = int(parts[1].strip())
                    if 1 <= weight <= 10:  # ê°€ì¤‘ì¹˜ ë²”ìœ„ ê²€ì¦
                        keywords_dict[keyword] = weight
                    else: # ê°€ì¤‘ì¹˜ ë²”ìœ„ ë²—ì–´ë‚¨
                        keywords_dict[keyword] = 5 
                except ValueError: # ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš°
                    keywords_dict[item] = 5
            else: # ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” ê²½ìš°
                keywords_dict[item] = 5
                
        return keywords_dict
        
    except Exception as e:
        st.error(f"ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {str(e)}. ì‘ë‹µ: '{response_text}'")
        return {}

def create_wordcloud_from_keywords(keywords_dict, width=800, height=600, bg_color='white'):
    """í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬ë¡œë¶€í„° ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    if not keywords_dict:
        return None
    
    # í°íŠ¸ ê²½ë¡œ í™•ì¸
    if not os.path.exists(FONT_PATH):
        st.error(f"í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FONT_PATH}")
        st.info("í°íŠ¸ íŒŒì¼ì´ /fonts/Pretendard-Bold.ttf ê²½ë¡œì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    
    try:
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        wc = WordCloud(
            font_path=FONT_PATH, # ìƒìˆ˜ ì‚¬ìš©
            width=width,
            height=height,
            background_color=bg_color,
            max_words=100,
            relative_scaling=0.5,
            min_font_size=10,
            colormap='viridis',
            prefer_horizontal=0.7
        ).generate_from_frequencies(keywords_dict)
        
        return wc
    except Exception as e:
        st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None

def wordcloud_to_image(wc, width=800, height=600):
    """ì›Œë“œí´ë¼ìš°ë“œë¥¼ PIL Imageë¡œ ë³€í™˜"""
    fig, ax = plt.subplots(figsize=(width/100, height/100)) # dpië¥¼ 100ìœ¼ë¡œ ê°€ì •í•˜ì—¬ figsize ê³„ì‚°
    ax.imshow(wc, interpolation='bilinear')
    ax.axis('off')
    
    buf = io.BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0, dpi=100)
    buf.seek(0)
    plt.close(fig) # ê·¸ë˜í”„ ë¦¬ì†ŒìŠ¤ í•´ì œ
    
    image = Image.open(buf)
    return image

# ì´ í•¨ìˆ˜ëŠ” analyze_network_metrics í•¨ìˆ˜ ë‚´ë¶€ì— ìˆì–´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
def get_image_download_link(img, filename, link_text):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±"""
    buffered = io.BytesIO()
    img.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    href = f'<a href="data:file/png;base64,{img_str}" download="{filename}">{link_text}</a>'
    return href


def create_network_analysis(text, keywords_dict, min_cooccurrence=1):
    """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ê°„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
    if not keywords_dict or len(keywords_dict) < 2:
        return None, None
    
    # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
    sentences = re.split(r'[.!?]\s*', text)
    
    # í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
    co_occurrence = Counter()
    keyword_list = list(keywords_dict.keys())
    
    for sentence in sentences:
        # ë¬¸ì¥ì— í¬í•¨ëœ í‚¤ì›Œë“œë“¤ ì°¾ê¸°
        found_keywords = []
        for keyword in keyword_list:
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì°¾ê¸° ìœ„í•´ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê±°ë‚˜ ì •ê·œì‹ ì‚¬ìš©
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ í‚¤ì›Œë“œ ìì²´ê°€ ë¬¸ì¥ì— ìˆëŠ”ì§€ í™•ì¸
            if keyword in sentence: 
                found_keywords.append(keyword)
        
        # ê°™ì€ ë¬¸ì¥ì— ë‚˜íƒ€ë‚œ í‚¤ì›Œë“œë“¤ì˜ ì¡°í•© ìƒì„±
        for combo in itertools.combinations(found_keywords, 2):
            # ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ (A,B)ì™€ (B,A)ë¥¼ ê°™ê²Œ ì²˜ë¦¬
            sorted_combo = tuple(sorted(combo))
            co_occurrence[sorted_combo] += 1
    
    # ìµœì†Œ ë™ì‹œì¶œí˜„ íšŸìˆ˜ í•„í„°ë§
    filtered_co_occurrence = {k: v for k, v in co_occurrence.items() if v >= min_cooccurrence}

    if not filtered_co_occurrence:
        return None, None
    
    # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
    G = nx.Graph()
    
    # í‚¤ì›Œë“œë¥¼ ë…¸ë“œë¡œ ì¶”ê°€ (ê°€ì¤‘ì¹˜ë¥¼ ë…¸ë“œ í¬ê¸°ë¡œ ì‚¬ìš©)
    # ë™ì‹œ ì¶œí˜„ëœ í‚¤ì›Œë“œë§Œ ë…¸ë“œë¡œ ì¶”ê°€ (ê³ ë¦½ ë…¸ë“œ ë°©ì§€)
    nodes_to_add = set()
    for (k1, k2) in filtered_co_occurrence.keys():
        nodes_to_add.add(k1)
        nodes_to_add.add(k2)

    for keyword in nodes_to_add:
        weight = keywords_dict.get(keyword, 1) # ì¶”ì¶œëœ í‚¤ì›Œë“œì— ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ 1
        G.add_node(keyword, weight=weight)
    
    # ë™ì‹œì¶œí˜„ì„ ì—£ì§€ë¡œ ì¶”ê°€
    for (keyword1, keyword2), freq in filtered_co_occurrence.items():
        G.add_edge(keyword1, keyword2, weight=freq)
    
    if G.number_of_nodes() < 2: # ìµœì†Œ 2ê°œ ë…¸ë“œ ì´ìƒì´ì–´ì•¼ ì˜ë¯¸ ìˆëŠ” ê·¸ë˜í”„
        return None, None

    return G, filtered_co_occurrence # í•„í„°ë§ëœ co_occurrence ë°˜í™˜

def draw_network_graph(G, keywords_dict):
    """ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°"""
    if G is None or len(G.nodes()) < 2:
        return None
    
    # í•œê¸€ í°íŠ¸ ì„¤ì •
    font_prop = None
    if os.path.exists(FONT_PATH): # FONT_PATH ìƒìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        font_prop = fm.FontProperties(fname=FONT_PATH)
    
    # ê·¸ë˜í”„ ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig, ax = plt.subplots(figsize=(12, 8)) # Streamlitì—ì„œ figë¥¼ ê´€ë¦¬í•´ì•¼ í•¨
    
    # ìŠ¤í”„ë§ ë ˆì´ì•„ì›ƒ ì‚¬ìš©
    # k ê°’ ì¡°ì •ìœ¼ë¡œ ë…¸ë“œ ê°„ ê°„ê²© ì¡°ì ˆ
    pos = nx.spring_layout(G, k=0.8 / np.sqrt(G.number_of_nodes()), iterations=50) 
    
    # ë…¸ë“œ í¬ê¸° ì„¤ì • (í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ê¸°ë°˜)
    node_sizes = [keywords_dict.get(node, 1) * 300 for node in G.nodes()] # ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì‚¬ìš©
    
    # ì—£ì§€ ë‘ê»˜ ì„¤ì • (ë™ì‹œì¶œí˜„ ë¹ˆë„ ê¸°ë°˜)
    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
    edge_widths = [w * 1.5 for w in edge_weights] # ì—£ì§€ ë‘ê»˜ ë°°ìœ¨ ì¡°ì •
    
    # ë…¸ë“œ ê·¸ë¦¬ê¸°
    nx.draw_networkx_nodes(G, pos, 
                          node_size=node_sizes, 
                          node_color='lightblue', 
                          alpha=0.7,
                          edgecolors='darkblue',
                          linewidths=1.5,
                          ax=ax) # ax ëª…ì‹œ
    
    # ì—£ì§€ ê·¸ë¦¬ê¸°
    nx.draw_networkx_edges(G, pos, 
                          width=edge_widths, 
                          alpha=0.6, 
                          edge_color='gray',
                          ax=ax) # ax ëª…ì‹œ
    
    # ë¼ë²¨ ê·¸ë¦¬ê¸°
    labels = {node: node for node in G.nodes()} # ëª¨ë“  ë…¸ë“œì— ë¼ë²¨ ì ìš©
    
    # í°íŠ¸ ì†ì„± ì ìš©
    if font_prop:
        nx.draw_networkx_labels(G, pos, 
                                labels=labels,
                               font_size=9, 
                               font_color='black',
                               font_weight='bold',
                               font_family=font_prop.get_name(), # <-- ì´ ë¶€ë¶„ì„ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤!
                               ax=ax) # ax ëª…ì‹œ
    else:
        nx.draw_networkx_labels(G, pos, 
                                labels=labels,
                               font_size=9, 
                               font_color='black',
                               font_weight='bold',
                               ax=ax) # ax ëª…ì‹œ
    
    ax.set_title('í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„', 
              fontproperties=font_prop if font_prop else None, 
              fontsize=16, 
              fontweight='bold')
    ax.axis('off')
    plt.tight_layout()
    
    # ì´ë¯¸ì§€ë¡œ ë³€í™˜
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')
    buf.seek(0)
    image = Image.open(buf)
    plt.close(fig) # ê·¸ë˜í”„ ë¦¬ì†ŒìŠ¤ í•´ì œ
    
    return image

def analyze_network_metrics(G, keywords_dict):
    """ë„¤íŠ¸ì›Œí¬ ì§€í‘œ ë¶„ì„"""
    if G is None or len(G.nodes()) < 2:
        return {}
    
    metrics = {}
    
    # ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ ì •ë³´
    metrics['nodes'] = G.number_of_nodes()
    metrics['edges'] = G.number_of_edges()
    
    # connected componentsëŠ” ê·¸ë˜í”„ê°€ ë¶„ë¦¬ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    if nx.is_connected(G):
        metrics['density'] = nx.density(G)
        metrics['components'] = 1
    else:
        metrics['density'] = "ê·¸ë˜í”„ê°€ ì—°ê²°ë˜ì–´ ìˆì§€ ì•ŠìŒ (ë°€ë„ ê³„ì‚° ë¶ˆê°€)"
        metrics['components'] = nx.number_connected_components(G)
    
    # ì¤‘ì‹¬ì„± ì§€í‘œ ê³„ì‚°
    try:
        # ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” ê·¸ë˜í”„ì— ëŒ€í•œ ì¤‘ì‹¬ì„±
        degree_centrality = nx.degree_centrality(G)
        betweenness_centrality = nx.betweenness_centrality(G)
        closeness_centrality = nx.closeness_centrality(G)
        
        metrics['most_connected'] = max(degree_centrality, key=degree_centrality.get) if degree_centrality else "N/A"
        metrics['most_between'] = max(betweenness_centrality, key=betweenness_centrality.get) if betweenness_centrality else "N/A"
        metrics['most_close'] = max(closeness_centrality, key=closeness_centrality.get) if closeness_centrality else "N/A"
        
        # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œì˜ ì¤‘ì‹¬ì„± ì ìˆ˜
        metrics['degree_top5'] = dict(sorted(degree_centrality.items(), 
                                           key=lambda x: x[1], reverse=True)[:5])
        metrics['betweenness_top5'] = dict(sorted(betweenness_centrality.items(), 
                                                key=lambda x: x[1], reverse=True)[:5])
        metrics['closeness_top5'] = dict(sorted(closeness_centrality.items(), 
                                                key=lambda x: x[1], reverse=True)[:5])
        
    except Exception as e:
        st.warning(f"ë„¤íŠ¸ì›Œí¬ ì¤‘ì‹¬ì„± ì§€í‘œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        metrics['most_connected'] = "ê³„ì‚° ë¶ˆê°€"
        metrics['most_between'] = "ê³„ì‚° ë¶ˆê°€"
        metrics['most_close'] = "ê³„ì‚° ë¶ˆê°€"
        metrics['degree_top5'] = {}
        metrics['betweenness_top5'] = {}
        metrics['closeness_top5'] = {}
    
    return metrics


# ì•± ì œëª© ë° ì„¤ëª…
st.title("ğŸ¤– GPT API í‚¤ì›Œë“œ ì¶”ì¶œ & ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°")
st.markdown("GPT APIë¥¼ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤!")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("ğŸ”‘ API ì„¤ì •")

# API í‚¤ ì…ë ¥
api_key = st.sidebar.text_input(
    "OpenAI API Key", 
    type="password",
    help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (https://platform.openai.com/api-keys)"
)

# GPT ëª¨ë¸ ì„ íƒ (ìœ íš¨í•œ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë³€ê²½)
model_choice = st.sidebar.selectbox(
    "GPT ëª¨ë¸",
    ["gpt-4o-mini", "gpt-3.5-turbo"], # ìœ íš¨í•œ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë³€ê²½
    index=0,
    help="ì‚¬ìš©í•  GPT ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”. gpt-4o-miniê°€ ê°€ì¥ ê²½ì œì ì´ë©´ì„œë„ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤."
)

# ì›Œë“œí´ë¼ìš°ë“œ ì„¤ì •
st.sidebar.header("ğŸ¨ ì‹œê°í™” ì„¤ì •")
wc_width = st.sidebar.slider("ì›Œë“œí´ë¼ìš°ë“œ ë„ˆë¹„", 400, 1200, 800)
wc_height = st.sidebar.slider("ì›Œë“œí´ë¼ìš°ë“œ ë†’ì´", 300, 800, 600)
bg_color = st.sidebar.selectbox("ë°°ê²½ìƒ‰", ["white", "black"], index=0)

# ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì„¤ì •
show_network = st.sidebar.checkbox("ğŸŒ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í¬í•¨", value=True)
min_cooccurrence = st.sidebar.slider("ìµœì†Œ ë™ì‹œì¶œí˜„ íšŸìˆ˜", 1, 5, 1, 
                                    help="ì´ ê°’ ì´ìƒìœ¼ë¡œ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œë“¤ë§Œ ì—°ê²°ì„ ìœ¼ë¡œ í‘œì‹œ")

# API í‚¤ í™•ì¸
if not api_key:
    st.warning("âš ï¸ OpenAI API í‚¤ë¥¼ ì™¼ìª½ ì‚¬ì´ë“œë°”ì— ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.info("""
    **API í‚¤ ë°œê¸‰ ë°©ë²•:**
    1. https://platform.openai.com ì ‘ì†
    2. ë¡œê·¸ì¸ í›„ 'API Keys' ë©”ë‰´ë¡œ ì´ë™
    3. 'Create new secret key' í´ë¦­
    4. ìƒì„±ëœ í‚¤ë¥¼ ë³µì‚¬í•˜ì—¬ ì‚¬ì´ë“œë°”ì— ì…ë ¥
    
    **ì£¼ì˜ì‚¬í•­:**
    - API ì‚¬ìš©ë£Œê°€ ë¶€ê³¼ë©ë‹ˆë‹¤
    - í‚¤ëŠ” ì•ˆì „í•˜ê²Œ ë³´ê´€í•˜ì„¸ìš”
    
    **í°íŠ¸ íŒŒì¼ ì„¤ì •:**
    - fonts/Pretendard-Bold.ttf íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤. /fonts ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•˜ê³  ê·¸ ì•ˆì— ë„£ì–´ì£¼ì„¸ìš”.
    """)

# ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
st.header("ğŸ“ í…ìŠ¤íŠ¸ ì…ë ¥")

# ì‚¬ìš©ë²• ì•ˆë‚´
with st.expander("ğŸ’¡ ì‚¬ìš©ë²• ë° ì˜ˆì‹œ"):
    st.markdown("""
    **ì‚¬ìš©ë²•:**
    1. OpenAI API í‚¤ë¥¼ ì‚¬ì´ë“œë°”ì— ì…ë ¥
    2. ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥
    3. 'GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ' ë²„íŠ¼ í´ë¦­
    4. ì¶”ì¶œëœ í‚¤ì›Œë“œ, ì›Œë“œí´ë¼ìš°ë“œ ë° ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ê²°ê³¼ í™•ì¸
    
    **ì˜ˆì‹œ í…ìŠ¤íŠ¸:**
    
ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ ë¶„ì•¼ì…ë‹ˆë‹¤. 
    ë°ì´í„° ê³¼í•™ìë“¤ì€ ë¹…ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ íŒ¨í„´ì„ ì°¾ê³ , 
    ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. 
    ìì—°ì–´ ì²˜ë¦¬ì™€ ì»´í“¨í„° ë¹„ì „ ê¸°ìˆ ì´ ë°œì „í•˜ë©´ì„œ 
    ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì—ì„œ í˜ì‹ ì´ ì¼ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤.

    
    **GPTì˜ ì¥ì :**
    - ë¬¸ë§¥ì„ ì´í•´í•œ ì •í™•í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
    - ë™ì˜ì–´/ìœ ì˜ì–´ ê·¸ë£¹í•‘
    - ì¤‘ìš”ë„ì— ë”°ë¥¸ ì •êµí•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    - ë³µí•©ì–´ì™€ ì „ë¬¸ìš©ì–´ ì¸ì‹
    """)

# í…ìŠ¤íŠ¸ ì…ë ¥ ì˜ì—­
text_input = st.text_area(
    "ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
    height=200,
    placeholder="ì˜ˆì‹œ: ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ ë¶„ì•¼ì…ë‹ˆë‹¤...",
    help="GPTê°€ ì´ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."
)

# í‚¤ì›Œë“œ ì¶”ì¶œ ë²„íŠ¼
if st.button("ğŸ¤– GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ", type="primary", disabled=not api_key):
    if not api_key:
        st.error("OpenAI API í‚¤ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not text_input.strip():
        st.warning("ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner(f"{model_choice} ëª¨ë¸ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
            # GPT API í˜¸ì¶œ
            gpt_response = call_openai_api(text_input, api_key, model_choice)
            
            if gpt_response.startswith("API ì˜¤ë¥˜") or gpt_response.startswith("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜") or gpt_response.startswith("ì˜¤ë¥˜ ë°œìƒ"):
                st.error(gpt_response)
            else:
                # GPT ì‘ë‹µ í‘œì‹œ
                st.success("í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
                
                st.header("ğŸ¤– GPT ì‘ë‹µ")
                st.code(gpt_response, language=None)
                
                # í‚¤ì›Œë“œ íŒŒì‹±
                keywords_dict = parse_gpt_response(gpt_response)
                
                if keywords_dict:
                    st.header("ğŸ“Š ì¶”ì¶œëœ í‚¤ì›Œë“œ")
                    
                    # í‚¤ì›Œë“œ ì •ë³´ í‘œì‹œ
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.subheader("ğŸ¯ í‚¤ì›Œë“œ ëª©ë¡")
                        # ê°€ì¤‘ì¹˜ê°€ 0ì¸ í‚¤ì›Œë“œëŠ” ì œì™¸í•˜ê³  í‘œì‹œ (parse_gpt_responseì—ì„œ 0ì´ ë  ì¼ì€ ì—†ì§€ë§Œ, ì•ˆì „í•˜ê²Œ)
                        display_keywords = {k: v for k, v in keywords_dict.items() if v > 0}
                        if display_keywords:
                            for keyword, weight in sorted(display_keywords.items(), key=lambda x: x[1], reverse=True):
                                st.write(f"**{keyword}**: {weight}")
                        else:
                            st.info("ì¶”ì¶œëœ ìœ íš¨í•œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

                    with col2:
                        st.subheader("ğŸ“ˆ ê°€ì¤‘ì¹˜ ë¶„í¬")
                        weights = list(keywords_dict.values())
                        
                        fig, ax = plt.subplots(figsize=(6, 4))
                        ax.hist(weights, bins=range(1, 12), alpha=0.7, color='skyblue', edgecolor='black')
                        
                        # í•œê¸€ í°íŠ¸ ì ìš© (setup_korean_fontì—ì„œ ì„¤ì •í–ˆìœ¼ë¯€ë¡œ font_familyëŠ” ì „ì—­ìœ¼ë¡œ ì ìš©ë˜ì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ fontpropertiesë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ)
                        if os.path.exists(FONT_PATH):
                            font_prop = fm.FontProperties(fname=FONT_PATH)
                            ax.set_xlabel('ê°€ì¤‘ì¹˜', fontproperties=font_prop)
                            ax.set_ylabel('í‚¤ì›Œë“œ ìˆ˜', fontproperties=font_prop)
                            ax.set_title('í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶„í¬', fontproperties=font_prop)
                        else:
                            ax.set_xlabel('Weight')
                            ax.set_ylabel('Keywords Count')
                            ax.set_title('Keyword Weight Distribution')
                            
                        ax.set_xticks(range(1, 11))
                        ax.grid(True, alpha=0.3)
                        st.pyplot(fig)
                        plt.close(fig) # ê·¸ë˜í”„ ë¦¬ì†ŒìŠ¤ í•´ì œ
                    
                    # ë³µì‚¬ ê°€ëŠ¥í•œ ê²°ê³¼
                    st.subheader("ğŸ“‹ ë³µì‚¬ìš© ê²°ê³¼")
                    st.text_area(
                        "ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹", 
                        value=gpt_response, 
                        height=100,
                        help="ì´ í…ìŠ¤íŠ¸ë¥¼ ë³µì‚¬í•´ì„œ ë‹¤ë¥¸ ì›Œë“œí´ë¼ìš°ë“œ ë„êµ¬ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )
                    
                    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
                    st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
                    
                    with st.spinner("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ëŠ” ì¤‘..."):
                        wordcloud = create_wordcloud_from_keywords(keywords_dict, wc_width, wc_height, bg_color)
                        
                        if wordcloud:
                            # ì›Œë“œí´ë¼ìš°ë“œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                            img = wordcloud_to_image(wordcloud, wc_width, wc_height)
                            
                            # ì´ë¯¸ì§€ í‘œì‹œ
                            st.image(img, caption="GPTë¡œ ìƒì„±ëœ ì›Œë“œí´ë¼ìš°ë“œ", use_column_width=True)
                            
                            # ë‹¤ìš´ë¡œë“œ ë§í¬ (ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œ)
                            download_link = get_image_download_link(img, "gpt_wordcloud.png", "ğŸ“¥ ì›Œë“œí´ë¼ìš°ë“œ ë‹¤ìš´ë¡œë“œ")
                            st.markdown(download_link, unsafe_allow_html=True)
                            
                            # ìƒì„± ì •ë³´
                            st.info(f"ëª¨ë¸: {model_choice} | í¬ê¸°: {wc_width}x{wc_height} | í‚¤ì›Œë“œ ìˆ˜: {len(keywords_dict)}ê°œ")
                
                else:
                    st.error("GPT ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

                # ğŸŒ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì„¹ì…˜ (ìƒˆë¡œ ì¶”ê°€ë˜ê±°ë‚˜ ìˆ˜ì •ëœ ë¶€ë¶„)
                if show_network:
                    st.header("ğŸŒ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
                    if len(keywords_dict) < 2:
                        st.warning("ë„¤íŠ¸ì›Œí¬ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ í‚¤ì›Œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                    else:
                        with st.spinner("í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¶„ì„í•˜ê³  ê·¸ë¦¬ëŠ” ì¤‘..."):
                            G, co_occurrence_data = create_network_analysis(text_input, keywords_dict, min_cooccurrence)
                            
                            if G and G.number_of_nodes() >= 2:
                                network_img = draw_network_graph(G, keywords_dict)
                                if network_img:
                                    st.image(network_img, caption="í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„", use_column_width=True)
                                    network_download_link = get_image_download_link(network_img, "keyword_network.png", "ğŸ“¥ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ë‹¤ìš´ë¡œë“œ")
                                    st.markdown(network_download_link, unsafe_allow_html=True)

                                    st.subheader("ğŸ“Š ë„¤íŠ¸ì›Œí¬ ì§€í‘œ")
                                    metrics = analyze_network_metrics(G, keywords_dict)
                                    if metrics:
                                        st.write(f"**ì´ ë…¸ë“œ ìˆ˜ (í‚¤ì›Œë“œ):** {metrics.get('nodes', 'N/A')}ê°œ")
                                        st.write(f"**ì´ ì—£ì§€ ìˆ˜ (ì—°ê²°):** {metrics.get('edges', 'N/A')}ê°œ")
                                        st.write(f"**ê·¸ë˜í”„ ë°€ë„:** {metrics.get('density', 'N/A'):.4f}" if isinstance(metrics.get('density'), float) else f"**ê·¸ë˜í”„ ë°€ë„:** {metrics.get('density', 'N/A')}")
                                        st.write(f"**ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸ ìˆ˜:** {metrics.get('components', 'N/A')}ê°œ")
                                        
                                        st.markdown("---")
                                        st.subheader("â­ ì£¼ìš” í‚¤ì›Œë“œ (ì¤‘ì‹¬ì„± ì§€í‘œ)")
                                        st.markdown("**ì—°ê²° ì¤‘ì‹¬ì„± (Degree Centrality):** ê°€ì¥ ë§ì€ í‚¤ì›Œë“œì™€ ì—°ê²°ëœ í‚¤ì›Œë“œ (ì˜í–¥ë ¥)")
                                        if metrics.get('degree_top5'):
                                            for k, v in metrics['degree_top5'].items():
                                                st.write(f"- **{k}**: {v:.4f}")
                                        else:
                                            st.write("ì •ë³´ ì—†ìŒ")

                                        st.markdown("**ë§¤ê°œ ì¤‘ì‹¬ì„± (Betweenness Centrality):** ë‹¤ë¥¸ í‚¤ì›Œë“œë“¤ ì‚¬ì´ì˜ ë‹¤ë¦¬ ì—­í• ì„ í•˜ëŠ” í‚¤ì›Œë“œ (ì¤‘ê°œì ì—­í• )")
                                        if metrics.get('betweenness_top5'):
                                            for k, v in metrics['betweenness_top5'].items():
                                                st.write(f"- **{k}**: {v:.4f}")
                                        else:
                                            st.write("ì •ë³´ ì—†ìŒ")
                                            
                                        st.markdown("**ê·¼ì ‘ ì¤‘ì‹¬ì„± (Closeness Centrality):** ë‹¤ë¥¸ í‚¤ì›Œë“œì— ì–¼ë§ˆë‚˜ ê°€ê¹ê²Œ ë„ë‹¬í•  ìˆ˜ ìˆëŠ”ì§€ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œ (ì •ë³´ í™•ì‚° ìš©ì´ì„±)")
                                        if metrics.get('closeness_top5'):
                                            for k, v in metrics['closeness_top5'].items():
                                                st.write(f"- **{k}**: {v:.4f}")
                                        else:
                                            st.write("ì •ë³´ ì—†ìŒ")

                                    st.subheader("â†”ï¸ í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¹ˆë„ (ìƒìœ„ 10ê°œ)")
                                    if co_occurrence_data:
                                        sorted_co_occurrence = sorted(co_occurrence_data.items(), key=lambda item: item[1], reverse=True)[:10]
                                        for (k1, k2), freq in sorted_co_occurrence:
                                            st.write(f"- **{k1}**ì™€ **{k2}**: {freq}íšŒ")
                                    else:
                                        st.info("ë™ì‹œì¶œí˜„ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ìµœì†Œ ë™ì‹œì¶œí˜„ íšŸìˆ˜ ì„¤ì •ì´ ë„ˆë¬´ ë†’ìŠµë‹ˆë‹¤.")
                                else:
                                    st.error("ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                            else:
                                st.info(f"ë„¤íŠ¸ì›Œí¬ë¥¼ ê·¸ë¦´ ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ í‚¤ì›Œë“œ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤. (í˜„ì¬ ë…¸ë“œ ìˆ˜: {G.number_of_nodes() if G else 0})")
                                if G and G.number_of_nodes() > 0:
                                    st.info(f"ì„ íƒëœ ìµœì†Œ ë™ì‹œì¶œí˜„ íšŸìˆ˜ ({min_cooccurrence}íšŒ) ì´ìƒìœ¼ë¡œ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì„¤ì •ì„ ë‚®ì¶°ë³´ì„¸ìš”.")

# ë¹„ìš© ì•ˆë‚´
if api_key:
    st.sidebar.markdown("---")
    st.sidebar.header("ğŸ’° ë¹„ìš© ì•ˆë‚´")
    st.sidebar.markdown("""
    **ì˜ˆìƒ ë¹„ìš© (1íšŒ ìš”ì²­):**
    - gpt-4o-mini: ~$0.000004/token (ì…ë ¥), ~$0.000012/token (ì¶œë ¥)
    - gpt-3.5-turbo: ~$0.0000005/token (ì…ë ¥), ~$0.0000015/token (ì¶œë ¥)
    
    *ì‹¤ì œ ë¹„ìš©ì€ ì…ë ¥ ë° ì¶œë ¥ í† í° ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.*
    *gpt-4o-miniê°€ gpt-3.5-turboë³´ë‹¤ ë¹„ì‹¸ì§€ë§Œ, í›¨ì”¬ ë” ë†’ì€ í’ˆì§ˆì˜ í‚¤ì›Œë“œ ì¶”ì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.*
    """)


# í‘¸í„°
st.markdown("---")
st.markdown("ğŸ’¡ **íŒ**: GPTê°€ ë¬¸ë§¥ì„ ì´í•´í•˜ë¯€ë¡œ ë” ì •í™•í•˜ê³  ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
st.markdown("ğŸ¤– GPT API í‚¤ì›Œë“œ ì¶”ì¶œê¸° | Made with Streamlit")